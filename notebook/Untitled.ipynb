{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919ec3c9-e170-414e-ac2e-4bf69b7b3e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Aelectron%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=all:electron&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</id>\n",
      "  <updated>2024-10-31T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">216342</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/cond-mat/0102536v1</id>\n",
      "    <updated>2001-02-28T20:12:09Z</updated>\n",
      "    <published>2001-02-28T20:12:09Z</published>\n",
      "    <title>Impact of Electron-Electron Cusp on Configuration Interaction Energies</title>\n",
      "    <summary>  The effect of the electron-electron cusp on the convergence of configuration\n",
      "interaction (CI) wave functions is examined. By analogy with the\n",
      "pseudopotential approach for electron-ion interactions, an effective\n",
      "electron-electron interaction is developed which closely reproduces the\n",
      "scattering of the Coulomb interaction but is smooth and finite at zero\n",
      "electron-electron separation. The exact many-electron wave function for this\n",
      "smooth effective interaction has no cusp at zero electron-electron separation.\n",
      "We perform CI and quantum Monte Carlo calculations for He and Be atoms, both\n",
      "with the Coulomb electron-electron interaction and with the smooth effective\n",
      "electron-electron interaction. We find that convergence of the CI expansion of\n",
      "the wave function for the smooth electron-electron interaction is not\n",
      "significantly improved compared with that for the divergent Coulomb interaction\n",
      "for energy differences on the order of 1 mHartree. This shows that, contrary to\n",
      "popular belief, description of the electron-electron cusp is not a limiting\n",
      "factor, to within chemical accuracy, for CI calculations.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>David Prendergast</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>M. Nolan</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Claudia Filippi</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stephen Fahy</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>J. C. Greer</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.1383585</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.1383585\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 6 figures, 3 tables, LaTeX209, submitted to The Journal of\n",
      "  Chemical Physics</arxiv:comment>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">J. Chem. Phys. 115, 1626 (2001)</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/cond-mat/0102536v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/0102536v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib, urllib.request\n",
    "url = 'http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=1'\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be903a12-ba9f-4113-97e8-76acbfdadd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_arxiv_paper_as_html(url):\n",
    "\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        return f\"Failed. Status code: {response.status_code}\"\n",
    "\n",
    "    else:\n",
    "        # Get the content of the web page\n",
    "        page_content = response.text\n",
    "\n",
    "        return page_content\n",
    "    \n",
    "    \n",
    "# Example\n",
    "\n",
    "html_url = 'https://arxiv.org/html/2406.04692v1'\n",
    "\n",
    "page_content = get_arxiv_paper_as_html(html_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bae246a-c0dd-44c0-bc57-6db19eb5f2de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\\n<title>Mixture-of-Agents Enhances Large Language Model Capabilities</title>\\n<!--Generated on Fri Jun  7 06:57:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\\n<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/ar5iv.0.7.9.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/latexml_styles.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/addons_new.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\\n<base href=\"/html/2406.04692v1/\"/></head>\\n<body>\\n<nav class=\"ltx_page_navbar\">\\n<nav class=\"ltx_TOC\">\\n<ol class=\"ltx_toclist\">\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S1\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">1 </span>Introduction</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S2\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2 </span>Mixture-of-Agents Methodology</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S2.SS1\" title=\"In 2 Mixture-of-Agents Methodology ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1 </span>Collaborativeness of LLMs</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S2.SS2\" title=\"In 2 Mixture-of-Agents Methodology ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2 </span>Mixture-of-Agents</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S2.SS3\" title=\"In 2 Mixture-of-Agents Methodology ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.3 </span>Analogy to Mixture-of-Experts</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3 </span>Evaluation</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS1\" title=\"In 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1 </span>Setup</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS1.SSS0.Px1\" title=\"In 3.1 Setup ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Benchmarks</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS1.SSS0.Px2\" title=\"In 3.1 Setup ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Models</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS2\" title=\"In 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.2 </span>Benchmark Results</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS2.SSS0.Px1\" title=\"In 3.2 Benchmark Results ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">AlpacaEval 2.0</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS2.SSS0.Px2\" title=\"In 3.2 Benchmark Results ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">MT-Bench</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS2.SSS0.Px3\" title=\"In 3.2 Benchmark Results ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">FLASK</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3\" title=\"In 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.3 </span>What Makes Mixture-of-Agents Work Well?</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3.SSS0.Px1\" title=\"In 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Mixture-of-Agents significantly outperforms LLM rankers.</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3.SSS0.Px2\" title=\"In 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">MoA tends to incorporate the best proposed answers.</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3.SSS0.Px3\" title=\"In 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Effect of model diversity and the number of proposers.</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3.SSS0.Px4\" title=\"In 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Specialization of models in the Mixture-of-Agent ecosystem.</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS4\" title=\"In 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.4 </span>Budget and Token Analysis</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS4.SSS0.Px1\" title=\"In 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Cost Effectiveness</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS4.SSS0.Px2\" title=\"In 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Tflops Consumption</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S4\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4 </span>Related Work</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S4.SS1\" title=\"In 4 Related Work ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1 </span>LLM Reasoning</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S4.SS2\" title=\"In 4 Related Work ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2 </span>Model Ensemble</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S5\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5 </span>Conclusion</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S5.SS0.SSS0.Px1\" title=\"In 5 Conclusion ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Limitations.</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S5.SS0.SSS0.Px2\" title=\"In 5 Conclusion ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\">Broader Impact.</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A1\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A </span>Spearman Correlation using Different Similarity Functions</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A2\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">B </span>LLM Ranker</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A3\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">C </span>Case Study</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A4\" title=\"In Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D </span>MATH Task</span></a></li>\\n</ol></nav>\\n</nav>\\n<div class=\"ltx_page_main\">\\n<div class=\"ltx_page_content\">\\n<article class=\"ltx_document ltx_authors_1line\">\\n<h1 class=\"ltx_title ltx_title_document\">Mixture-of-Agents Enhances Large Language Model Capabilities</h1>\\n<div class=\"ltx_authors\">\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">Junlin Wang \\n<br class=\"ltx_break\"/>Duke University \\n<br class=\"ltx_break\"/>Together AI \\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\" id=\"id1.1.id1\">junlin.wang2@duke.edu</span>\\n<br class=\"ltx_break\"/>&amp;Jue Wang \\n<br class=\"ltx_break\"/>Together AI \\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\" id=\"id2.2.id2\">jue@together.ai</span>\\n<br class=\"ltx_break\"/>&amp;Ben Athiwaratkun \\n<br class=\"ltx_break\"/>Together AI \\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\" id=\"id3.3.id3\">ben@together.ai</span>\\n<br class=\"ltx_break\"/>&amp;Ce Zhang \\n<br class=\"ltx_break\"/>University of Chicago \\n<br class=\"ltx_break\"/>Together AI \\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\" id=\"id4.4.id4\">cez@uchicago.edu</span>\\n<br class=\"ltx_break\"/>&amp;James Zou \\n<br class=\"ltx_break\"/>Stanford University \\n<br class=\"ltx_break\"/>Together AI\\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\" id=\"id5.5.id5\">jamesz@stanford.edu</span>\\n<br class=\"ltx_break\"/>\\n</span></span>\\n</div>\\n<div class=\"ltx_abstract\">\\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\\n<p class=\"ltx_p\" id=\"id6.id1\">Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks.\\nWith the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction.\\nToward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology.\\nIn our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents.\\nEach agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response.\\nMoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>\\nOur code can be found in: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/togethercomputer/moa\" title=\"\">https://github.com/togethercomputer/moa</a>.\\n</span></span></span></p>\\n</div>\\n<section class=\"ltx_section\" id=\"S1\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\\n<div class=\"ltx_para\" id=\"S1.p1\">\\n<p class=\"ltx_p\" id=\"S1.p1.1\">Large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib42\" title=\"\">2022a</a>; Chowdhery et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib6\" title=\"\">2022</a>; Touvron et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib29\" title=\"\">2023a</a>; Team et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib27\" title=\"\">2023</a>; Brown et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib2\" title=\"\">2020</a>; OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib20\" title=\"\">2023</a>)</cite>\\nhave significantly advanced the field of natural language understanding and generation in recent years.\\nThese models are pretrained on vast amounts of data and subsequently aligned with human preferences to generate helpful and coherent outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib21\" title=\"\">2022</a>)</cite>.\\nHowever, despite the plethora of LLMs and their impressive achievements,\\nthey still face inherent constraints on model size and training data.\\nFurther scaling up these models is exceptionally costly, often requiring extensive retraining on several trillion tokens.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p2\">\\n<p class=\"ltx_p\" id=\"S1.p2.1\">At the same time, different LLMs possess unique strengths and specialize in various tasks aspects.\\nFor instance, some models excel at complex instruction following <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib36\" title=\"\">2023a</a>)</cite>\\nwhile others may be better suited for code generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Roziere et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib24\" title=\"\">2023</a>; Guo et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib11\" title=\"\">2024</a>)</cite>.\\nThis diversity in skill sets among different LLMs presents an intriguing question:\\n<span class=\"ltx_text ltx_font_italic\" id=\"S1.p2.1.1\">Can we harness the collective expertise of multiple LLMs to create a more capable and robust model?</span></p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p3\">\\n<p class=\"ltx_p\" id=\"S1.p3.1\">Our answer to this question is <span class=\"ltx_text ltx_font_italic\" id=\"S1.p3.1.1\">Yes</span>.\\nWe identify an inherent phenomenon we term the <span class=\"ltx_text ltx_font_italic\" id=\"S1.p3.1.2\">collaborativeness</span> of LLMs\\n— wherein an LLM tends to generate better responses when presented with outputs from other models, even if these other models are less capable by itself.\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S1.F1\" title=\"In 1 Introduction ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a> showcases the LC win rate on the AlpacaEval 2.0\\nbenchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubois et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib9\" title=\"\">2024</a>)</cite> for 6 popular LLMs.</p>\\n</div>\\n<figure class=\"ltx_figure ltx_align_floatright\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"583\" id=\"S1.F1.g1\" src=\"x1.png\" width=\"1196\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S1.F1.2.1.1\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" id=\"S1.F1.3.2\" style=\"font-size:90%;\">AlpacaEval 2.0 LC win rates improve when provided with responses from other models.</span></figcaption>\\n</figure>\\n<div class=\"ltx_para\" id=\"S1.p4\">\\n<p class=\"ltx_p\" id=\"S1.p4.1\">When these models are provided with answers generated independently by these models,\\ntheir LC win rates significantly improve.\\nThis indicates that the collaborativeness phenomenon is widespread among LLMs.\\nRemarkably, this improvement occurs even when the auxiliary responses provided by the other models are of lower quality than what an individual LLM could generate independently.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p5\">\\n<p class=\"ltx_p\" id=\"S1.p5.2\">Based on this finding, this paper introduces a Mixture-of-Agents (MoA) methodology\\nthat leverages multiple LLMs to iteratively enhance the generation quality.\\nThe structure of MoA is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S1.F2\" title=\"In 1 Introduction ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\\nInitially, LLMs in the first layer, denoted as agents <math alttext=\"A_{1,1},...A_{1,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.1.m1.6\"><semantics id=\"S1.p5.1.m1.6a\"><mrow id=\"S1.p5.1.m1.6.6.2\" xref=\"S1.p5.1.m1.6.6.3.cmml\"><msub id=\"S1.p5.1.m1.5.5.1.1\" xref=\"S1.p5.1.m1.5.5.1.1.cmml\"><mi id=\"S1.p5.1.m1.5.5.1.1.2\" xref=\"S1.p5.1.m1.5.5.1.1.2.cmml\">A</mi><mrow id=\"S1.p5.1.m1.2.2.2.4\" xref=\"S1.p5.1.m1.2.2.2.3.cmml\"><mn id=\"S1.p5.1.m1.1.1.1.1\" xref=\"S1.p5.1.m1.1.1.1.1.cmml\">1</mn><mo id=\"S1.p5.1.m1.2.2.2.4.1\" xref=\"S1.p5.1.m1.2.2.2.3.cmml\">,</mo><mn id=\"S1.p5.1.m1.2.2.2.2\" xref=\"S1.p5.1.m1.2.2.2.2.cmml\">1</mn></mrow></msub><mo id=\"S1.p5.1.m1.6.6.2.3\" xref=\"S1.p5.1.m1.6.6.3.cmml\">,</mo><mrow id=\"S1.p5.1.m1.6.6.2.2\" xref=\"S1.p5.1.m1.6.6.2.2.cmml\"><mi id=\"S1.p5.1.m1.6.6.2.2.2\" mathvariant=\"normal\" xref=\"S1.p5.1.m1.6.6.2.2.2.cmml\">…</mi><mo id=\"S1.p5.1.m1.6.6.2.2.1\" xref=\"S1.p5.1.m1.6.6.2.2.1.cmml\">\\u2062</mo><msub id=\"S1.p5.1.m1.6.6.2.2.3\" xref=\"S1.p5.1.m1.6.6.2.2.3.cmml\"><mi id=\"S1.p5.1.m1.6.6.2.2.3.2\" xref=\"S1.p5.1.m1.6.6.2.2.3.2.cmml\">A</mi><mrow id=\"S1.p5.1.m1.4.4.2.4\" xref=\"S1.p5.1.m1.4.4.2.3.cmml\"><mn id=\"S1.p5.1.m1.3.3.1.1\" xref=\"S1.p5.1.m1.3.3.1.1.cmml\">1</mn><mo id=\"S1.p5.1.m1.4.4.2.4.1\" xref=\"S1.p5.1.m1.4.4.2.3.cmml\">,</mo><mi id=\"S1.p5.1.m1.4.4.2.2\" xref=\"S1.p5.1.m1.4.4.2.2.cmml\">n</mi></mrow></msub></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.p5.1.m1.6b\"><list id=\"S1.p5.1.m1.6.6.3.cmml\" xref=\"S1.p5.1.m1.6.6.2\"><apply id=\"S1.p5.1.m1.5.5.1.1.cmml\" xref=\"S1.p5.1.m1.5.5.1.1\"><csymbol cd=\"ambiguous\" id=\"S1.p5.1.m1.5.5.1.1.1.cmml\" xref=\"S1.p5.1.m1.5.5.1.1\">subscript</csymbol><ci id=\"S1.p5.1.m1.5.5.1.1.2.cmml\" xref=\"S1.p5.1.m1.5.5.1.1.2\">𝐴</ci><list id=\"S1.p5.1.m1.2.2.2.3.cmml\" xref=\"S1.p5.1.m1.2.2.2.4\"><cn id=\"S1.p5.1.m1.1.1.1.1.cmml\" type=\"integer\" xref=\"S1.p5.1.m1.1.1.1.1\">1</cn><cn id=\"S1.p5.1.m1.2.2.2.2.cmml\" type=\"integer\" xref=\"S1.p5.1.m1.2.2.2.2\">1</cn></list></apply><apply id=\"S1.p5.1.m1.6.6.2.2.cmml\" xref=\"S1.p5.1.m1.6.6.2.2\"><times id=\"S1.p5.1.m1.6.6.2.2.1.cmml\" xref=\"S1.p5.1.m1.6.6.2.2.1\"></times><ci id=\"S1.p5.1.m1.6.6.2.2.2.cmml\" xref=\"S1.p5.1.m1.6.6.2.2.2\">…</ci><apply id=\"S1.p5.1.m1.6.6.2.2.3.cmml\" xref=\"S1.p5.1.m1.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S1.p5.1.m1.6.6.2.2.3.1.cmml\" xref=\"S1.p5.1.m1.6.6.2.2.3\">subscript</csymbol><ci id=\"S1.p5.1.m1.6.6.2.2.3.2.cmml\" xref=\"S1.p5.1.m1.6.6.2.2.3.2\">𝐴</ci><list id=\"S1.p5.1.m1.4.4.2.3.cmml\" xref=\"S1.p5.1.m1.4.4.2.4\"><cn id=\"S1.p5.1.m1.3.3.1.1.cmml\" type=\"integer\" xref=\"S1.p5.1.m1.3.3.1.1\">1</cn><ci id=\"S1.p5.1.m1.4.4.2.2.cmml\" xref=\"S1.p5.1.m1.4.4.2.2\">𝑛</ci></list></apply></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p5.1.m1.6c\">A_{1,1},...A_{1,n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p5.1.m1.6d\">italic_A start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT , … italic_A start_POSTSUBSCRIPT 1 , italic_n end_POSTSUBSCRIPT</annotation></semantics></math> independently generate responses to a given prompt.\\nThese responses are then presented to agents in the next layer <math alttext=\"A_{2,1},...A_{2,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.2.m2.6\"><semantics id=\"S1.p5.2.m2.6a\"><mrow id=\"S1.p5.2.m2.6.6.2\" xref=\"S1.p5.2.m2.6.6.3.cmml\"><msub id=\"S1.p5.2.m2.5.5.1.1\" xref=\"S1.p5.2.m2.5.5.1.1.cmml\"><mi id=\"S1.p5.2.m2.5.5.1.1.2\" xref=\"S1.p5.2.m2.5.5.1.1.2.cmml\">A</mi><mrow id=\"S1.p5.2.m2.2.2.2.4\" xref=\"S1.p5.2.m2.2.2.2.3.cmml\"><mn id=\"S1.p5.2.m2.1.1.1.1\" xref=\"S1.p5.2.m2.1.1.1.1.cmml\">2</mn><mo id=\"S1.p5.2.m2.2.2.2.4.1\" xref=\"S1.p5.2.m2.2.2.2.3.cmml\">,</mo><mn id=\"S1.p5.2.m2.2.2.2.2\" xref=\"S1.p5.2.m2.2.2.2.2.cmml\">1</mn></mrow></msub><mo id=\"S1.p5.2.m2.6.6.2.3\" xref=\"S1.p5.2.m2.6.6.3.cmml\">,</mo><mrow id=\"S1.p5.2.m2.6.6.2.2\" xref=\"S1.p5.2.m2.6.6.2.2.cmml\"><mi id=\"S1.p5.2.m2.6.6.2.2.2\" mathvariant=\"normal\" xref=\"S1.p5.2.m2.6.6.2.2.2.cmml\">…</mi><mo id=\"S1.p5.2.m2.6.6.2.2.1\" xref=\"S1.p5.2.m2.6.6.2.2.1.cmml\">\\u2062</mo><msub id=\"S1.p5.2.m2.6.6.2.2.3\" xref=\"S1.p5.2.m2.6.6.2.2.3.cmml\"><mi id=\"S1.p5.2.m2.6.6.2.2.3.2\" xref=\"S1.p5.2.m2.6.6.2.2.3.2.cmml\">A</mi><mrow id=\"S1.p5.2.m2.4.4.2.4\" xref=\"S1.p5.2.m2.4.4.2.3.cmml\"><mn id=\"S1.p5.2.m2.3.3.1.1\" xref=\"S1.p5.2.m2.3.3.1.1.cmml\">2</mn><mo id=\"S1.p5.2.m2.4.4.2.4.1\" xref=\"S1.p5.2.m2.4.4.2.3.cmml\">,</mo><mi id=\"S1.p5.2.m2.4.4.2.2\" xref=\"S1.p5.2.m2.4.4.2.2.cmml\">n</mi></mrow></msub></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.p5.2.m2.6b\"><list id=\"S1.p5.2.m2.6.6.3.cmml\" xref=\"S1.p5.2.m2.6.6.2\"><apply id=\"S1.p5.2.m2.5.5.1.1.cmml\" xref=\"S1.p5.2.m2.5.5.1.1\"><csymbol cd=\"ambiguous\" id=\"S1.p5.2.m2.5.5.1.1.1.cmml\" xref=\"S1.p5.2.m2.5.5.1.1\">subscript</csymbol><ci id=\"S1.p5.2.m2.5.5.1.1.2.cmml\" xref=\"S1.p5.2.m2.5.5.1.1.2\">𝐴</ci><list id=\"S1.p5.2.m2.2.2.2.3.cmml\" xref=\"S1.p5.2.m2.2.2.2.4\"><cn id=\"S1.p5.2.m2.1.1.1.1.cmml\" type=\"integer\" xref=\"S1.p5.2.m2.1.1.1.1\">2</cn><cn id=\"S1.p5.2.m2.2.2.2.2.cmml\" type=\"integer\" xref=\"S1.p5.2.m2.2.2.2.2\">1</cn></list></apply><apply id=\"S1.p5.2.m2.6.6.2.2.cmml\" xref=\"S1.p5.2.m2.6.6.2.2\"><times id=\"S1.p5.2.m2.6.6.2.2.1.cmml\" xref=\"S1.p5.2.m2.6.6.2.2.1\"></times><ci id=\"S1.p5.2.m2.6.6.2.2.2.cmml\" xref=\"S1.p5.2.m2.6.6.2.2.2\">…</ci><apply id=\"S1.p5.2.m2.6.6.2.2.3.cmml\" xref=\"S1.p5.2.m2.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S1.p5.2.m2.6.6.2.2.3.1.cmml\" xref=\"S1.p5.2.m2.6.6.2.2.3\">subscript</csymbol><ci id=\"S1.p5.2.m2.6.6.2.2.3.2.cmml\" xref=\"S1.p5.2.m2.6.6.2.2.3.2\">𝐴</ci><list id=\"S1.p5.2.m2.4.4.2.3.cmml\" xref=\"S1.p5.2.m2.4.4.2.4\"><cn id=\"S1.p5.2.m2.3.3.1.1.cmml\" type=\"integer\" xref=\"S1.p5.2.m2.3.3.1.1\">2</cn><ci id=\"S1.p5.2.m2.4.4.2.2.cmml\" xref=\"S1.p5.2.m2.4.4.2.2\">𝑛</ci></list></apply></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p5.2.m2.6c\">A_{2,1},...A_{2,n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p5.2.m2.6d\">italic_A start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT , … italic_A start_POSTSUBSCRIPT 2 , italic_n end_POSTSUBSCRIPT</annotation></semantics></math>\\n(which may reuse a model from the first layer) for further refinement.\\nThis iterative refinement process continues for several cycles until obtaining a more robust and comprehensive response.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S1.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"199\" id=\"S1.F2.g1\" src=\"x2.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S1.F2.2.1.1\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" id=\"S1.F2.3.2\" style=\"font-size:90%;\">Illustration of the Mixture-of-Agents Structure.\\nThis example showcases 4 MoA layers with 3 agents in each layer.\\nThe agents here can share the same model.\\n</span></figcaption>\\n</figure>\\n<div class=\"ltx_para\" id=\"S1.p6\">\\n<p class=\"ltx_p\" id=\"S1.p6.2\">To ensure effective collaboration among models and improve overall response quality,\\ncareful selection of LLMs for each MoA layer is crucial.\\nThis selection process is guided by two primary criteria:\\n(a) Performance Metrics: The average win rate of models in layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.1.m1.1\"><semantics id=\"S1.p6.1.m1.1a\"><mi id=\"S1.p6.1.m1.1.1\" xref=\"S1.p6.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.p6.1.m1.1b\"><ci id=\"S1.p6.1.m1.1.1.cmml\" xref=\"S1.p6.1.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p6.1.m1.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p6.1.m1.1d\">italic_i</annotation></semantics></math> plays a significant role in determining their suitability for inclusion in layer <math alttext=\"i+1\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.2.m2.1\"><semantics id=\"S1.p6.2.m2.1a\"><mrow id=\"S1.p6.2.m2.1.1\" xref=\"S1.p6.2.m2.1.1.cmml\"><mi id=\"S1.p6.2.m2.1.1.2\" xref=\"S1.p6.2.m2.1.1.2.cmml\">i</mi><mo id=\"S1.p6.2.m2.1.1.1\" xref=\"S1.p6.2.m2.1.1.1.cmml\">+</mo><mn id=\"S1.p6.2.m2.1.1.3\" xref=\"S1.p6.2.m2.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.p6.2.m2.1b\"><apply id=\"S1.p6.2.m2.1.1.cmml\" xref=\"S1.p6.2.m2.1.1\"><plus id=\"S1.p6.2.m2.1.1.1.cmml\" xref=\"S1.p6.2.m2.1.1.1\"></plus><ci id=\"S1.p6.2.m2.1.1.2.cmml\" xref=\"S1.p6.2.m2.1.1.2\">𝑖</ci><cn id=\"S1.p6.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"S1.p6.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p6.2.m2.1c\">i+1</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p6.2.m2.1d\">italic_i + 1</annotation></semantics></math>. Therefore, selecting models based on their demonstrated performance metrics ensures higher-quality outputs.\\n(b) Diversity Considerations: The diversity of model outputs is also crucial.\\nResponses generated by heterogeneous models contribute significantly more than those produced by the same model as we show later in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3\" title=\"3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">section</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.\\nBy leveraging these criteria — performance and diversity\\n— MoA aims to mitigate individual model deficiencies and enhance overall response quality through collaborative synthesis.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p7\">\\n<p class=\"ltx_p\" id=\"S1.p7.1\">We conduct comprehensive evaluations using AlpacaEval 2.0, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib44\" title=\"\">2023</a>)</cite>, FLASK <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib40\" title=\"\">2023</a>)</cite> benchmarks for assessing the response quality across various dimensions. The results demonstrate substantial improvements with our proposed method, achieving a new SOTA win rate of 65.8% on AlpacaEval 2.0 compared to the previous best of 57.5% achieved by GPT-4 Omni.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S1.p8\">\\n<p class=\"ltx_p\" id=\"S1.p8.1\">The <span class=\"ltx_text ltx_font_bold\" id=\"S1.p8.1.1\">contributions</span> of this work are summarized as follows:\\n(1) <span class=\"ltx_text ltx_font_italic\" id=\"S1.p8.1.2\">Novel framework</span>: we propose a Mixture-of-Agents framework designed to leverage the strengths of multiple LLMs, thereby improving their reasoning and language generation capabilities.\\n(2) <span class=\"ltx_text ltx_font_italic\" id=\"S1.p8.1.3\">Finding of collaborativeness of language models</span>: we highlight the inherit collaborativeness among LLMs, where models tend to generate better quality responses when they have access to outputs from other models, even if those outputs are of lower quality.\\n(3) <span class=\"ltx_text ltx_font_italic\" id=\"S1.p8.1.4\">State-of-the-art LLM performance</span>: we conducted extensive experiments using multiple highly-competitive benchmarks such as AlpacaEval 2.0, MT-Bench, and FLASK; our MoA framework achieves state-of-the-art performance on these benchmarks.</p>\\n</div>\\n</section>\\n<section class=\"ltx_section\" id=\"S2\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Mixture-of-Agents Methodology</h2>\\n<div class=\"ltx_para\" id=\"S2.p1\">\\n<p class=\"ltx_p\" id=\"S2.p1.1\">In this section, we present our proposed methodology for leveraging multiple models to achieve boosted performance.\\nWe begin by demonstrating that LLMs possess collaborativeness and thus can improve their responses based on the outputs of other models.\\nFollowing this, we introduce the Mixture-of-Agents methodology and discuss its design implications.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span>Collaborativeness of LLMs</h3>\\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p1.1\">We begin by demonstrating the collaborativeness of LLMs,\\nspecifically their ability to generate higher quality responses\\nwhen they can reference outputs from other models.\\nAs we have shown in the introduction and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S1.F1\" title=\"In 1 Introduction ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a>,\\nmany of today’s available LLMs exhibit this collaborative capability.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p2.1\">An important pathway to extract maximum benefits from collaboration of multiple LLMs is to characterize how different models are good at in various aspects of collaboration.\\nDuring the collaboration process, we can categorize LLMs into two distinct roles:</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.SS1.p3.1.1\">Proposers</span> excel at generating useful reference responses for use by other models.\\nWhile a good proposer may not necessarily produce responses with high scores by itself,\\nit should offer more context and diverse perspectives, ultimately contributing to better final responses when used by an aggregator.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p4\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.SS1.p4.1.1\">Aggregators</span> are models proficient in synthesizing responses from other models into a single, high-quality output.\\nAn effective aggregator should maintain or enhance output quality even when integrating inputs that are of lesser quality than its own.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p5\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p5.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.SS3\" title=\"3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">3.3</span></a> empirically validate the roles of aggregators and proposers.\\nSpecifically, we show that many LLMs possess capabilities both as aggregators and proposers,\\nwhile certain models displayed specialized proficiencies in distinct roles.\\nGPT-4o, Qwen1.5, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks.\\nIn contrast, WizardLM demonstrated excellent performance as an proposer model\\nbut struggled to maintain its effectiveness in aggregating responses from other models.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS1.p6\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p6.1\">Given that an aggregator can generate higher-quality responses by building upon outputs from other models,\\nwe propose further enhancing this collaborative potential by introducing additional aggregators.\\nOne intuitive idea is to replicate the exercise with multiple aggregators\\n— initially using several to aggregate better answers and then re-aggregating these aggregated answers.\\nBy incorporating more aggregators into the process, we can iteratively synthesize and refine the responses,\\nleveraging the strengths of multiple models to produce superior outcomes.\\nThis leads to the design of our proposed Mixture-of-Agents.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span>Mixture-of-Agents</h3>\\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p1.6\">The structure of MoA is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S1.F2\" title=\"In 1 Introduction ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\\nIt has <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.1.m1.1\"><semantics id=\"S2.SS2.p1.1.m1.1a\"><mi id=\"S2.SS2.p1.1.m1.1.1\" xref=\"S2.SS2.p1.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.1.m1.1b\"><ci id=\"S2.SS2.p1.1.m1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.1.1\">𝑙</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.1.m1.1c\">l</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.1.m1.1d\">italic_l</annotation></semantics></math> layers and each layer-<math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.2.m2.1\"><semantics id=\"S2.SS2.p1.2.m2.1a\"><mi id=\"S2.SS2.p1.2.m2.1.1\" xref=\"S2.SS2.p1.2.m2.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.2.m2.1b\"><ci id=\"S2.SS2.p1.2.m2.1.1.cmml\" xref=\"S2.SS2.p1.2.m2.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.2.m2.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.2.m2.1d\">italic_i</annotation></semantics></math> consists of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.3.m3.1\"><semantics id=\"S2.SS2.p1.3.m3.1a\"><mi id=\"S2.SS2.p1.3.m3.1.1\" xref=\"S2.SS2.p1.3.m3.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.3.m3.1b\"><ci id=\"S2.SS2.p1.3.m3.1.1.cmml\" xref=\"S2.SS2.p1.3.m3.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.3.m3.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.3.m3.1d\">italic_n</annotation></semantics></math> LLMs, denoted by <math alttext=\"A_{i,1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.4.m4.2\"><semantics id=\"S2.SS2.p1.4.m4.2a\"><msub id=\"S2.SS2.p1.4.m4.2.3\" xref=\"S2.SS2.p1.4.m4.2.3.cmml\"><mi id=\"S2.SS2.p1.4.m4.2.3.2\" xref=\"S2.SS2.p1.4.m4.2.3.2.cmml\">A</mi><mrow id=\"S2.SS2.p1.4.m4.2.2.2.4\" xref=\"S2.SS2.p1.4.m4.2.2.2.3.cmml\"><mi id=\"S2.SS2.p1.4.m4.1.1.1.1\" xref=\"S2.SS2.p1.4.m4.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS2.p1.4.m4.2.2.2.4.1\" xref=\"S2.SS2.p1.4.m4.2.2.2.3.cmml\">,</mo><mn id=\"S2.SS2.p1.4.m4.2.2.2.2\" xref=\"S2.SS2.p1.4.m4.2.2.2.2.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.4.m4.2b\"><apply id=\"S2.SS2.p1.4.m4.2.3.cmml\" xref=\"S2.SS2.p1.4.m4.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p1.4.m4.2.3.1.cmml\" xref=\"S2.SS2.p1.4.m4.2.3\">subscript</csymbol><ci id=\"S2.SS2.p1.4.m4.2.3.2.cmml\" xref=\"S2.SS2.p1.4.m4.2.3.2\">𝐴</ci><list id=\"S2.SS2.p1.4.m4.2.2.2.3.cmml\" xref=\"S2.SS2.p1.4.m4.2.2.2.4\"><ci id=\"S2.SS2.p1.4.m4.1.1.1.1.cmml\" xref=\"S2.SS2.p1.4.m4.1.1.1.1\">𝑖</ci><cn id=\"S2.SS2.p1.4.m4.2.2.2.2.cmml\" type=\"integer\" xref=\"S2.SS2.p1.4.m4.2.2.2.2\">1</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.4.m4.2c\">A_{i,1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.4.m4.2d\">italic_A start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext=\"A_{i,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.5.m5.2\"><semantics id=\"S2.SS2.p1.5.m5.2a\"><msub id=\"S2.SS2.p1.5.m5.2.3\" xref=\"S2.SS2.p1.5.m5.2.3.cmml\"><mi id=\"S2.SS2.p1.5.m5.2.3.2\" xref=\"S2.SS2.p1.5.m5.2.3.2.cmml\">A</mi><mrow id=\"S2.SS2.p1.5.m5.2.2.2.4\" xref=\"S2.SS2.p1.5.m5.2.2.2.3.cmml\"><mi id=\"S2.SS2.p1.5.m5.1.1.1.1\" xref=\"S2.SS2.p1.5.m5.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS2.p1.5.m5.2.2.2.4.1\" xref=\"S2.SS2.p1.5.m5.2.2.2.3.cmml\">,</mo><mn id=\"S2.SS2.p1.5.m5.2.2.2.2\" xref=\"S2.SS2.p1.5.m5.2.2.2.2.cmml\">2</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.5.m5.2b\"><apply id=\"S2.SS2.p1.5.m5.2.3.cmml\" xref=\"S2.SS2.p1.5.m5.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p1.5.m5.2.3.1.cmml\" xref=\"S2.SS2.p1.5.m5.2.3\">subscript</csymbol><ci id=\"S2.SS2.p1.5.m5.2.3.2.cmml\" xref=\"S2.SS2.p1.5.m5.2.3.2\">𝐴</ci><list id=\"S2.SS2.p1.5.m5.2.2.2.3.cmml\" xref=\"S2.SS2.p1.5.m5.2.2.2.4\"><ci id=\"S2.SS2.p1.5.m5.1.1.1.1.cmml\" xref=\"S2.SS2.p1.5.m5.1.1.1.1\">𝑖</ci><cn id=\"S2.SS2.p1.5.m5.2.2.2.2.cmml\" type=\"integer\" xref=\"S2.SS2.p1.5.m5.2.2.2.2\">2</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.5.m5.2c\">A_{i,2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.5.m5.2d\">italic_A start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT</annotation></semantics></math>, …, <math alttext=\"A_{i,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.6.m6.2\"><semantics id=\"S2.SS2.p1.6.m6.2a\"><msub id=\"S2.SS2.p1.6.m6.2.3\" xref=\"S2.SS2.p1.6.m6.2.3.cmml\"><mi id=\"S2.SS2.p1.6.m6.2.3.2\" xref=\"S2.SS2.p1.6.m6.2.3.2.cmml\">A</mi><mrow id=\"S2.SS2.p1.6.m6.2.2.2.4\" xref=\"S2.SS2.p1.6.m6.2.2.2.3.cmml\"><mi id=\"S2.SS2.p1.6.m6.1.1.1.1\" xref=\"S2.SS2.p1.6.m6.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS2.p1.6.m6.2.2.2.4.1\" xref=\"S2.SS2.p1.6.m6.2.2.2.3.cmml\">,</mo><mi id=\"S2.SS2.p1.6.m6.2.2.2.2\" xref=\"S2.SS2.p1.6.m6.2.2.2.2.cmml\">n</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.6.m6.2b\"><apply id=\"S2.SS2.p1.6.m6.2.3.cmml\" xref=\"S2.SS2.p1.6.m6.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p1.6.m6.2.3.1.cmml\" xref=\"S2.SS2.p1.6.m6.2.3\">subscript</csymbol><ci id=\"S2.SS2.p1.6.m6.2.3.2.cmml\" xref=\"S2.SS2.p1.6.m6.2.3.2\">𝐴</ci><list id=\"S2.SS2.p1.6.m6.2.2.2.3.cmml\" xref=\"S2.SS2.p1.6.m6.2.2.2.4\"><ci id=\"S2.SS2.p1.6.m6.1.1.1.1.cmml\" xref=\"S2.SS2.p1.6.m6.1.1.1.1\">𝑖</ci><ci id=\"S2.SS2.p1.6.m6.2.2.2.2.cmml\" xref=\"S2.SS2.p1.6.m6.2.2.2.2\">𝑛</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.6.m6.2c\">A_{i,n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.6.m6.2d\">italic_A start_POSTSUBSCRIPT italic_i , italic_n end_POSTSUBSCRIPT</annotation></semantics></math>.\\nIt is important to note that LLMs can be reused either within the same layer or across different layers.\\nWhen many LLMs in a layer are identical,\\nthis configuration leads to a special structure that corresponds to a model generating multiple possibly different outputs (due to the stochasticity of temperature sampling).\\nWe refer to this setting as single-proposer, where only a sparse subset of models are activated.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p2.4\">Here, each LLM <math alttext=\"A_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.1.m1.2\"><semantics id=\"S2.SS2.p2.1.m1.2a\"><msub id=\"S2.SS2.p2.1.m1.2.3\" xref=\"S2.SS2.p2.1.m1.2.3.cmml\"><mi id=\"S2.SS2.p2.1.m1.2.3.2\" xref=\"S2.SS2.p2.1.m1.2.3.2.cmml\">A</mi><mrow id=\"S2.SS2.p2.1.m1.2.2.2.4\" xref=\"S2.SS2.p2.1.m1.2.2.2.3.cmml\"><mi id=\"S2.SS2.p2.1.m1.1.1.1.1\" xref=\"S2.SS2.p2.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS2.p2.1.m1.2.2.2.4.1\" xref=\"S2.SS2.p2.1.m1.2.2.2.3.cmml\">,</mo><mi id=\"S2.SS2.p2.1.m1.2.2.2.2\" xref=\"S2.SS2.p2.1.m1.2.2.2.2.cmml\">j</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.1.m1.2b\"><apply id=\"S2.SS2.p2.1.m1.2.3.cmml\" xref=\"S2.SS2.p2.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p2.1.m1.2.3.1.cmml\" xref=\"S2.SS2.p2.1.m1.2.3\">subscript</csymbol><ci id=\"S2.SS2.p2.1.m1.2.3.2.cmml\" xref=\"S2.SS2.p2.1.m1.2.3.2\">𝐴</ci><list id=\"S2.SS2.p2.1.m1.2.2.2.3.cmml\" xref=\"S2.SS2.p2.1.m1.2.2.2.4\"><ci id=\"S2.SS2.p2.1.m1.1.1.1.1.cmml\" xref=\"S2.SS2.p2.1.m1.1.1.1.1\">𝑖</ci><ci id=\"S2.SS2.p2.1.m1.2.2.2.2.cmml\" xref=\"S2.SS2.p2.1.m1.2.2.2.2\">𝑗</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.1.m1.2c\">A_{i,j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.1.m1.2d\">italic_A start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> processes an input text and generates its continuation.\\nOur method does not require any fine-tuning and only utilizes the interface of prompting and generation of LLMs.\\nFormally, given an input prompt <math alttext=\"x_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.2.m2.1\"><semantics id=\"S2.SS2.p2.2.m2.1a\"><msub id=\"S2.SS2.p2.2.m2.1.1\" xref=\"S2.SS2.p2.2.m2.1.1.cmml\"><mi id=\"S2.SS2.p2.2.m2.1.1.2\" xref=\"S2.SS2.p2.2.m2.1.1.2.cmml\">x</mi><mn id=\"S2.SS2.p2.2.m2.1.1.3\" xref=\"S2.SS2.p2.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.2.m2.1b\"><apply id=\"S2.SS2.p2.2.m2.1.1.cmml\" xref=\"S2.SS2.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p2.2.m2.1.1.1.cmml\" xref=\"S2.SS2.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S2.SS2.p2.2.m2.1.1.2.cmml\" xref=\"S2.SS2.p2.2.m2.1.1.2\">𝑥</ci><cn id=\"S2.SS2.p2.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"S2.SS2.p2.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.2.m2.1c\">x_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.2.m2.1d\">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the output of <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.3.m3.1\"><semantics id=\"S2.SS2.p2.3.m3.1a\"><mi id=\"S2.SS2.p2.3.m3.1.1\" xref=\"S2.SS2.p2.3.m3.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.3.m3.1b\"><ci id=\"S2.SS2.p2.3.m3.1.1.cmml\" xref=\"S2.SS2.p2.3.m3.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.3.m3.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.3.m3.1d\">italic_i</annotation></semantics></math>-th MoA layer <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.4.m4.1\"><semantics id=\"S2.SS2.p2.4.m4.1a\"><msub id=\"S2.SS2.p2.4.m4.1.1\" xref=\"S2.SS2.p2.4.m4.1.1.cmml\"><mi id=\"S2.SS2.p2.4.m4.1.1.2\" xref=\"S2.SS2.p2.4.m4.1.1.2.cmml\">y</mi><mi id=\"S2.SS2.p2.4.m4.1.1.3\" xref=\"S2.SS2.p2.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.4.m4.1b\"><apply id=\"S2.SS2.p2.4.m4.1.1.cmml\" xref=\"S2.SS2.p2.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p2.4.m4.1.1.1.cmml\" xref=\"S2.SS2.p2.4.m4.1.1\">subscript</csymbol><ci id=\"S2.SS2.p2.4.m4.1.1.2.cmml\" xref=\"S2.SS2.p2.4.m4.1.1.2\">𝑦</ci><ci id=\"S2.SS2.p2.4.m4.1.1.3.cmml\" xref=\"S2.SS2.p2.4.m4.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.4.m4.1c\">y_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.4.m4.1d\">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can be expressed as follows:</p>\\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A4.EGx1\">\\n<tbody id=\"S2.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\\\displaystyle y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.E1.m1.1\"><semantics id=\"S2.E1.m1.1a\"><msub id=\"S2.E1.m1.1.1\" xref=\"S2.E1.m1.1.1.cmml\"><mi id=\"S2.E1.m1.1.1.2\" xref=\"S2.E1.m1.1.1.2.cmml\">y</mi><mi id=\"S2.E1.m1.1.1.3\" xref=\"S2.E1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.E1.m1.1b\"><apply id=\"S2.E1.m1.1.1.cmml\" xref=\"S2.E1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m1.1.1.1.cmml\" xref=\"S2.E1.m1.1.1\">subscript</csymbol><ci id=\"S2.E1.m1.1.1.2.cmml\" xref=\"S2.E1.m1.1.1.2\">𝑦</ci><ci id=\"S2.E1.m1.1.1.3.cmml\" xref=\"S2.E1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E1.m1.1c\">\\\\displaystyle y_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.E1.m1.1d\">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>\\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\\\displaystyle=\\\\oplus_{j=1}^{n}[A_{i,j}(x_{i})]\\\\,\\\\,+x_{1},x_{i+1}=y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.E1.m2.4\"><semantics id=\"S2.E1.m2.4a\"><mrow id=\"S2.E1.m2.4.4.2\" xref=\"S2.E1.m2.4.4.3.cmml\"><mrow id=\"S2.E1.m2.3.3.1.1\" xref=\"S2.E1.m2.3.3.1.1.cmml\"><mi id=\"S2.E1.m2.3.3.1.1.3\" xref=\"S2.E1.m2.3.3.1.1.3.cmml\"></mi><mo id=\"S2.E1.m2.3.3.1.1.2\" rspace=\"0em\" xref=\"S2.E1.m2.3.3.1.1.2.cmml\">=</mo><mrow id=\"S2.E1.m2.3.3.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.cmml\"><mrow id=\"S2.E1.m2.3.3.1.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.cmml\"><msubsup id=\"S2.E1.m2.3.3.1.1.1.1.2\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.cmml\"><mo id=\"S2.E1.m2.3.3.1.1.1.1.2.2.2\" lspace=\"0em\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.2.cmml\">⊕</mo><mrow id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.cmml\"><mi id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.2\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.2.cmml\">j</mi><mo id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.1.cmml\">=</mo><mn id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.3\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.3.cmml\">1</mn></mrow><mi id=\"S2.E1.m2.3.3.1.1.1.1.2.3\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.3.cmml\">n</mi></msubsup><mrow id=\"S2.E1.m2.3.3.1.1.1.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.2.cmml\"><mo id=\"S2.E1.m2.3.3.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.2.1.cmml\">[</mo><mrow id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.cmml\"><msub id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.2\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.2.cmml\">A</mi><mrow id=\"S2.E1.m2.2.2.2.4\" xref=\"S2.E1.m2.2.2.2.3.cmml\"><mi id=\"S2.E1.m2.1.1.1.1\" xref=\"S2.E1.m2.1.1.1.1.cmml\">i</mi><mo id=\"S2.E1.m2.2.2.2.4.1\" xref=\"S2.E1.m2.2.2.2.3.cmml\">,</mo><mi id=\"S2.E1.m2.2.2.2.2\" xref=\"S2.E1.m2.2.2.2.2.cmml\">j</mi></mrow></msub><mo id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.2\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.2.cmml\">\\u2062</mo><mrow id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml\">x</mi><mi id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.3\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S2.E1.m2.3.3.1.1.1.1.1.1.3\" rspace=\"0.330em\" stretchy=\"false\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.2.1.cmml\">]</mo></mrow></mrow><mo id=\"S2.E1.m2.3.3.1.1.1.2\" xref=\"S2.E1.m2.3.3.1.1.1.2.cmml\">+</mo><msub id=\"S2.E1.m2.3.3.1.1.1.3\" xref=\"S2.E1.m2.3.3.1.1.1.3.cmml\"><mi id=\"S2.E1.m2.3.3.1.1.1.3.2\" xref=\"S2.E1.m2.3.3.1.1.1.3.2.cmml\">x</mi><mn id=\"S2.E1.m2.3.3.1.1.1.3.3\" xref=\"S2.E1.m2.3.3.1.1.1.3.3.cmml\">1</mn></msub></mrow></mrow><mo id=\"S2.E1.m2.4.4.2.3\" xref=\"S2.E1.m2.4.4.3a.cmml\">,</mo><mrow id=\"S2.E1.m2.4.4.2.2\" xref=\"S2.E1.m2.4.4.2.2.cmml\"><msub id=\"S2.E1.m2.4.4.2.2.2\" xref=\"S2.E1.m2.4.4.2.2.2.cmml\"><mi id=\"S2.E1.m2.4.4.2.2.2.2\" xref=\"S2.E1.m2.4.4.2.2.2.2.cmml\">x</mi><mrow id=\"S2.E1.m2.4.4.2.2.2.3\" xref=\"S2.E1.m2.4.4.2.2.2.3.cmml\"><mi id=\"S2.E1.m2.4.4.2.2.2.3.2\" xref=\"S2.E1.m2.4.4.2.2.2.3.2.cmml\">i</mi><mo id=\"S2.E1.m2.4.4.2.2.2.3.1\" xref=\"S2.E1.m2.4.4.2.2.2.3.1.cmml\">+</mo><mn id=\"S2.E1.m2.4.4.2.2.2.3.3\" xref=\"S2.E1.m2.4.4.2.2.2.3.3.cmml\">1</mn></mrow></msub><mo id=\"S2.E1.m2.4.4.2.2.1\" xref=\"S2.E1.m2.4.4.2.2.1.cmml\">=</mo><msub id=\"S2.E1.m2.4.4.2.2.3\" xref=\"S2.E1.m2.4.4.2.2.3.cmml\"><mi id=\"S2.E1.m2.4.4.2.2.3.2\" xref=\"S2.E1.m2.4.4.2.2.3.2.cmml\">y</mi><mi id=\"S2.E1.m2.4.4.2.2.3.3\" xref=\"S2.E1.m2.4.4.2.2.3.3.cmml\">i</mi></msub></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.E1.m2.4b\"><apply id=\"S2.E1.m2.4.4.3.cmml\" xref=\"S2.E1.m2.4.4.2\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.4.4.3a.cmml\" xref=\"S2.E1.m2.4.4.2.3\">formulae-sequence</csymbol><apply id=\"S2.E1.m2.3.3.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1\"><eq id=\"S2.E1.m2.3.3.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.2\"></eq><csymbol cd=\"latexml\" id=\"S2.E1.m2.3.3.1.1.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.3\">absent</csymbol><apply id=\"S2.E1.m2.3.3.1.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1\"><plus id=\"S2.E1.m2.3.3.1.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.2\"></plus><apply id=\"S2.E1.m2.3.3.1.1.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1\"><apply id=\"S2.E1.m2.3.3.1.1.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.3.3.1.1.1.1.2.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2\">superscript</csymbol><apply id=\"S2.E1.m2.3.3.1.1.1.1.2.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.3.3.1.1.1.1.2.2.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2\">subscript</csymbol><csymbol cd=\"latexml\" id=\"S2.E1.m2.3.3.1.1.1.1.2.2.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.2\">direct-sum</csymbol><apply id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3\"><eq id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.1\"></eq><ci id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.2\">𝑗</ci><cn id=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.3.cmml\" type=\"integer\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.2.3.3\">1</cn></apply></apply><ci id=\"S2.E1.m2.3.3.1.1.1.1.2.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.2.3\">𝑛</ci></apply><apply id=\"S2.E1.m2.3.3.1.1.1.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S2.E1.m2.3.3.1.1.1.1.1.2.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.2\">delimited-[]</csymbol><apply id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1\"><times id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.2\"></times><apply id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.3.2\">𝐴</ci><list id=\"S2.E1.m2.2.2.2.3.cmml\" xref=\"S2.E1.m2.2.2.2.4\"><ci id=\"S2.E1.m2.1.1.1.1.cmml\" xref=\"S2.E1.m2.1.1.1.1\">𝑖</ci><ci id=\"S2.E1.m2.2.2.2.2.cmml\" xref=\"S2.E1.m2.2.2.2.2\">𝑗</ci></list></apply><apply id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.2\">𝑥</ci><ci id=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.1.1.1.1.1.1.1.3\">𝑖</ci></apply></apply></apply></apply><apply id=\"S2.E1.m2.3.3.1.1.1.3.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.3.3.1.1.1.3.1.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.3\">subscript</csymbol><ci id=\"S2.E1.m2.3.3.1.1.1.3.2.cmml\" xref=\"S2.E1.m2.3.3.1.1.1.3.2\">𝑥</ci><cn id=\"S2.E1.m2.3.3.1.1.1.3.3.cmml\" type=\"integer\" xref=\"S2.E1.m2.3.3.1.1.1.3.3\">1</cn></apply></apply></apply><apply id=\"S2.E1.m2.4.4.2.2.cmml\" xref=\"S2.E1.m2.4.4.2.2\"><eq id=\"S2.E1.m2.4.4.2.2.1.cmml\" xref=\"S2.E1.m2.4.4.2.2.1\"></eq><apply id=\"S2.E1.m2.4.4.2.2.2.cmml\" xref=\"S2.E1.m2.4.4.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.4.4.2.2.2.1.cmml\" xref=\"S2.E1.m2.4.4.2.2.2\">subscript</csymbol><ci id=\"S2.E1.m2.4.4.2.2.2.2.cmml\" xref=\"S2.E1.m2.4.4.2.2.2.2\">𝑥</ci><apply id=\"S2.E1.m2.4.4.2.2.2.3.cmml\" xref=\"S2.E1.m2.4.4.2.2.2.3\"><plus id=\"S2.E1.m2.4.4.2.2.2.3.1.cmml\" xref=\"S2.E1.m2.4.4.2.2.2.3.1\"></plus><ci id=\"S2.E1.m2.4.4.2.2.2.3.2.cmml\" xref=\"S2.E1.m2.4.4.2.2.2.3.2\">𝑖</ci><cn id=\"S2.E1.m2.4.4.2.2.2.3.3.cmml\" type=\"integer\" xref=\"S2.E1.m2.4.4.2.2.2.3.3\">1</cn></apply></apply><apply id=\"S2.E1.m2.4.4.2.2.3.cmml\" xref=\"S2.E1.m2.4.4.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.E1.m2.4.4.2.2.3.1.cmml\" xref=\"S2.E1.m2.4.4.2.2.3\">subscript</csymbol><ci id=\"S2.E1.m2.4.4.2.2.3.2.cmml\" xref=\"S2.E1.m2.4.4.2.2.3.2\">𝑦</ci><ci id=\"S2.E1.m2.4.4.2.2.3.3.cmml\" xref=\"S2.E1.m2.4.4.2.2.3.3\">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E1.m2.4c\">\\\\displaystyle=\\\\oplus_{j=1}^{n}[A_{i,j}(x_{i})]\\\\,\\\\,+x_{1},x_{i+1}=y_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.E1.m2.4d\">= ⊕ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT [ italic_A start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] + italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>\\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\\n</tr></tbody>\\n</table>\\n<p class=\"ltx_p\" id=\"S2.SS2.p2.6\">where <math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.5.m1.1\"><semantics id=\"S2.SS2.p2.5.m1.1a\"><mo id=\"S2.SS2.p2.5.m1.1.1\" xref=\"S2.SS2.p2.5.m1.1.1.cmml\">+</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.5.m1.1b\"><plus id=\"S2.SS2.p2.5.m1.1.1.cmml\" xref=\"S2.SS2.p2.5.m1.1.1\"></plus></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.5.m1.1c\">+</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.5.m1.1d\">+</annotation></semantics></math> here means concatenation of texts; <math alttext=\"\\\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.6.m2.1\"><semantics id=\"S2.SS2.p2.6.m2.1a\"><mo id=\"S2.SS2.p2.6.m2.1.1\" xref=\"S2.SS2.p2.6.m2.1.1.cmml\">⊕</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p2.6.m2.1b\"><csymbol cd=\"latexml\" id=\"S2.SS2.p2.6.m2.1.1.cmml\" xref=\"S2.SS2.p2.6.m2.1.1\">direct-sum</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p2.6.m2.1c\">\\\\oplus</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p2.6.m2.1d\">⊕</annotation></semantics></math> means application of the Aggregate-and-Synthesize prompt shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S2.T1\" title=\"In 2.2 Mixture-of-Agents ‣ 2 Mixture-of-Agents Methodology ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a> to these model outputs.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S2.T1\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Aggregate-and-Synthesize Prompt to integrate responses from other models.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.4\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S2.T1.4.5.1\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S2.T1.4.5.1.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.5.1.1.1\">\\n<span class=\"ltx_p\" id=\"S2.T1.4.5.1.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.5.1.1.1.1.1\" style=\"font-size:90%;\">You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.4.6.2\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S2.T1.4.6.2.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.6.2.1.1\">\\n<span class=\"ltx_p\" id=\"S2.T1.4.6.2.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.6.2.1.1.1.1\" style=\"font-size:90%;\">Responses from models:</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S2.T1.1.1.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.1.1.1\">\\n<span class=\"ltx_p\" id=\"S2.T1.1.1.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"S2.T1.1.1.1.1.1.1\" style=\"font-size:90%;\">1. [Model Response from </span><math alttext=\"A_{i,1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.1.1.1.1.1.m1.2\"><semantics id=\"S2.T1.1.1.1.1.1.m1.2a\"><msub id=\"S2.T1.1.1.1.1.1.m1.2.3\" xref=\"S2.T1.1.1.1.1.1.m1.2.3.cmml\"><mi id=\"S2.T1.1.1.1.1.1.m1.2.3.2\" mathsize=\"90%\" xref=\"S2.T1.1.1.1.1.1.m1.2.3.2.cmml\">A</mi><mrow id=\"S2.T1.1.1.1.1.1.m1.2.2.2.4\" xref=\"S2.T1.1.1.1.1.1.m1.2.2.2.3.cmml\"><mi id=\"S2.T1.1.1.1.1.1.m1.1.1.1.1\" mathsize=\"90%\" xref=\"S2.T1.1.1.1.1.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.1.1.1.1.1.m1.2.2.2.4.1\" mathsize=\"90%\" xref=\"S2.T1.1.1.1.1.1.m1.2.2.2.3.cmml\">,</mo><mn id=\"S2.T1.1.1.1.1.1.m1.2.2.2.2\" mathsize=\"90%\" xref=\"S2.T1.1.1.1.1.1.m1.2.2.2.2.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.1.1.1.1.1.m1.2b\"><apply id=\"S2.T1.1.1.1.1.1.m1.2.3.cmml\" xref=\"S2.T1.1.1.1.1.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.1.1.1.1.1.m1.2.3.1.cmml\" xref=\"S2.T1.1.1.1.1.1.m1.2.3\">subscript</csymbol><ci id=\"S2.T1.1.1.1.1.1.m1.2.3.2.cmml\" xref=\"S2.T1.1.1.1.1.1.m1.2.3.2\">𝐴</ci><list id=\"S2.T1.1.1.1.1.1.m1.2.2.2.3.cmml\" xref=\"S2.T1.1.1.1.1.1.m1.2.2.2.4\"><ci id=\"S2.T1.1.1.1.1.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.1.1.1.1.1.m1.1.1.1.1\">𝑖</ci><cn id=\"S2.T1.1.1.1.1.1.m1.2.2.2.2.cmml\" type=\"integer\" xref=\"S2.T1.1.1.1.1.1.m1.2.2.2.2\">1</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.1.1.1.1.1.m1.2c\">A_{i,1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.T1.1.1.1.1.1.m1.2d\">italic_A start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT</annotation></semantics></math><span class=\"ltx_text\" id=\"S2.T1.1.1.1.1.1.2\" style=\"font-size:90%;\">]</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S2.T1.2.2.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.1.1\">\\n<span class=\"ltx_p\" id=\"S2.T1.2.2.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"S2.T1.2.2.1.1.1.1\" style=\"font-size:90%;\">2. [Model Response from </span><math alttext=\"A_{i,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.2.2.1.1.1.m1.2\"><semantics id=\"S2.T1.2.2.1.1.1.m1.2a\"><msub id=\"S2.T1.2.2.1.1.1.m1.2.3\" xref=\"S2.T1.2.2.1.1.1.m1.2.3.cmml\"><mi id=\"S2.T1.2.2.1.1.1.m1.2.3.2\" mathsize=\"90%\" xref=\"S2.T1.2.2.1.1.1.m1.2.3.2.cmml\">A</mi><mrow id=\"S2.T1.2.2.1.1.1.m1.2.2.2.4\" xref=\"S2.T1.2.2.1.1.1.m1.2.2.2.3.cmml\"><mi id=\"S2.T1.2.2.1.1.1.m1.1.1.1.1\" mathsize=\"90%\" xref=\"S2.T1.2.2.1.1.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.2.2.1.1.1.m1.2.2.2.4.1\" mathsize=\"90%\" xref=\"S2.T1.2.2.1.1.1.m1.2.2.2.3.cmml\">,</mo><mn id=\"S2.T1.2.2.1.1.1.m1.2.2.2.2\" mathsize=\"90%\" xref=\"S2.T1.2.2.1.1.1.m1.2.2.2.2.cmml\">2</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.2.2.1.1.1.m1.2b\"><apply id=\"S2.T1.2.2.1.1.1.m1.2.3.cmml\" xref=\"S2.T1.2.2.1.1.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.2.2.1.1.1.m1.2.3.1.cmml\" xref=\"S2.T1.2.2.1.1.1.m1.2.3\">subscript</csymbol><ci id=\"S2.T1.2.2.1.1.1.m1.2.3.2.cmml\" xref=\"S2.T1.2.2.1.1.1.m1.2.3.2\">𝐴</ci><list id=\"S2.T1.2.2.1.1.1.m1.2.2.2.3.cmml\" xref=\"S2.T1.2.2.1.1.1.m1.2.2.2.4\"><ci id=\"S2.T1.2.2.1.1.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.2.2.1.1.1.m1.1.1.1.1\">𝑖</ci><cn id=\"S2.T1.2.2.1.1.1.m1.2.2.2.2.cmml\" type=\"integer\" xref=\"S2.T1.2.2.1.1.1.m1.2.2.2.2\">2</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.2.2.1.1.1.m1.2c\">A_{i,2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.T1.2.2.1.1.1.m1.2d\">italic_A start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT</annotation></semantics></math><span class=\"ltx_text\" id=\"S2.T1.2.2.1.1.1.2\" style=\"font-size:90%;\">]</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.4.7.3\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S2.T1.4.7.3.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.7.3.1.1\">\\n<span class=\"ltx_p\" id=\"S2.T1.4.7.3.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.7.3.1.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S2.T1.4.4.2\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.2.2\">\\n<span class=\"ltx_p\" id=\"S2.T1.4.4.2.2.2\" style=\"width:433.6pt;\"><math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.3.3.1.1.1.m1.1\"><semantics id=\"S2.T1.3.3.1.1.1.m1.1a\"><mi id=\"S2.T1.3.3.1.1.1.m1.1.1\" mathsize=\"90%\" xref=\"S2.T1.3.3.1.1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.3.1.1.1.m1.1b\"><ci id=\"S2.T1.3.3.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.3.3.1.1.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.3.1.1.1.m1.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.T1.3.3.1.1.1.m1.1d\">italic_n</annotation></semantics></math><span class=\"ltx_text\" id=\"S2.T1.4.4.2.2.2.1\" style=\"font-size:90%;\">. [Model Response from </span><math alttext=\"A_{i,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.4.4.2.2.2.m2.2\"><semantics id=\"S2.T1.4.4.2.2.2.m2.2a\"><msub id=\"S2.T1.4.4.2.2.2.m2.2.3\" xref=\"S2.T1.4.4.2.2.2.m2.2.3.cmml\"><mi id=\"S2.T1.4.4.2.2.2.m2.2.3.2\" mathsize=\"90%\" xref=\"S2.T1.4.4.2.2.2.m2.2.3.2.cmml\">A</mi><mrow id=\"S2.T1.4.4.2.2.2.m2.2.2.2.4\" xref=\"S2.T1.4.4.2.2.2.m2.2.2.2.3.cmml\"><mi id=\"S2.T1.4.4.2.2.2.m2.1.1.1.1\" mathsize=\"90%\" xref=\"S2.T1.4.4.2.2.2.m2.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.4.4.2.2.2.m2.2.2.2.4.1\" mathsize=\"90%\" xref=\"S2.T1.4.4.2.2.2.m2.2.2.2.3.cmml\">,</mo><mi id=\"S2.T1.4.4.2.2.2.m2.2.2.2.2\" mathsize=\"90%\" xref=\"S2.T1.4.4.2.2.2.m2.2.2.2.2.cmml\">n</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.4.4.2.2.2.m2.2b\"><apply id=\"S2.T1.4.4.2.2.2.m2.2.3.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.4.4.2.2.2.m2.2.3.1.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.2.3\">subscript</csymbol><ci id=\"S2.T1.4.4.2.2.2.m2.2.3.2.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.2.3.2\">𝐴</ci><list id=\"S2.T1.4.4.2.2.2.m2.2.2.2.3.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.2.2.2.4\"><ci id=\"S2.T1.4.4.2.2.2.m2.1.1.1.1.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.1.1.1.1\">𝑖</ci><ci id=\"S2.T1.4.4.2.2.2.m2.2.2.2.2.cmml\" xref=\"S2.T1.4.4.2.2.2.m2.2.2.2.2\">𝑛</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.4.4.2.2.2.m2.2c\">A_{i,n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.T1.4.4.2.2.2.m2.2d\">italic_A start_POSTSUBSCRIPT italic_i , italic_n end_POSTSUBSCRIPT</annotation></semantics></math><span class=\"ltx_text\" id=\"S2.T1.4.4.2.2.2.2\" style=\"font-size:90%;\">]</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<div class=\"ltx_para\" id=\"S2.SS2.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p3.2\">In practice, we do not need to concatenate prompt and all model responses so only one LLM is needed to be used in the last layer.\\nTherefore, we use the output of an LLM from the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.1.m1.1\"><semantics id=\"S2.SS2.p3.1.m1.1a\"><mi id=\"S2.SS2.p3.1.m1.1.1\" xref=\"S2.SS2.p3.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p3.1.m1.1b\"><ci id=\"S2.SS2.p3.1.m1.1.1.cmml\" xref=\"S2.SS2.p3.1.m1.1.1\">𝑙</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p3.1.m1.1c\">l</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p3.1.m1.1d\">italic_l</annotation></semantics></math>-th layer (<math alttext=\"A_{l,1}(x_{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.2.m2.3\"><semantics id=\"S2.SS2.p3.2.m2.3a\"><mrow id=\"S2.SS2.p3.2.m2.3.3\" xref=\"S2.SS2.p3.2.m2.3.3.cmml\"><msub id=\"S2.SS2.p3.2.m2.3.3.3\" xref=\"S2.SS2.p3.2.m2.3.3.3.cmml\"><mi id=\"S2.SS2.p3.2.m2.3.3.3.2\" xref=\"S2.SS2.p3.2.m2.3.3.3.2.cmml\">A</mi><mrow id=\"S2.SS2.p3.2.m2.2.2.2.4\" xref=\"S2.SS2.p3.2.m2.2.2.2.3.cmml\"><mi id=\"S2.SS2.p3.2.m2.1.1.1.1\" xref=\"S2.SS2.p3.2.m2.1.1.1.1.cmml\">l</mi><mo id=\"S2.SS2.p3.2.m2.2.2.2.4.1\" xref=\"S2.SS2.p3.2.m2.2.2.2.3.cmml\">,</mo><mn id=\"S2.SS2.p3.2.m2.2.2.2.2\" xref=\"S2.SS2.p3.2.m2.2.2.2.2.cmml\">1</mn></mrow></msub><mo id=\"S2.SS2.p3.2.m2.3.3.2\" xref=\"S2.SS2.p3.2.m2.3.3.2.cmml\">\\u2062</mo><mrow id=\"S2.SS2.p3.2.m2.3.3.1.1\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.cmml\"><mo id=\"S2.SS2.p3.2.m2.3.3.1.1.2\" stretchy=\"false\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.cmml\">(</mo><msub id=\"S2.SS2.p3.2.m2.3.3.1.1.1\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.cmml\"><mi id=\"S2.SS2.p3.2.m2.3.3.1.1.1.2\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.2.cmml\">x</mi><mi id=\"S2.SS2.p3.2.m2.3.3.1.1.1.3\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.3.cmml\">l</mi></msub><mo id=\"S2.SS2.p3.2.m2.3.3.1.1.3\" stretchy=\"false\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p3.2.m2.3b\"><apply id=\"S2.SS2.p3.2.m2.3.3.cmml\" xref=\"S2.SS2.p3.2.m2.3.3\"><times id=\"S2.SS2.p3.2.m2.3.3.2.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.2\"></times><apply id=\"S2.SS2.p3.2.m2.3.3.3.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p3.2.m2.3.3.3.1.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.3\">subscript</csymbol><ci id=\"S2.SS2.p3.2.m2.3.3.3.2.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.3.2\">𝐴</ci><list id=\"S2.SS2.p3.2.m2.2.2.2.3.cmml\" xref=\"S2.SS2.p3.2.m2.2.2.2.4\"><ci id=\"S2.SS2.p3.2.m2.1.1.1.1.cmml\" xref=\"S2.SS2.p3.2.m2.1.1.1.1\">𝑙</ci><cn id=\"S2.SS2.p3.2.m2.2.2.2.2.cmml\" type=\"integer\" xref=\"S2.SS2.p3.2.m2.2.2.2.2\">1</cn></list></apply><apply id=\"S2.SS2.p3.2.m2.3.3.1.1.1.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.SS2.p3.2.m2.3.3.1.1.1.1.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.1.1\">subscript</csymbol><ci id=\"S2.SS2.p3.2.m2.3.3.1.1.1.2.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.2\">𝑥</ci><ci id=\"S2.SS2.p3.2.m2.3.3.1.1.1.3.cmml\" xref=\"S2.SS2.p3.2.m2.3.3.1.1.1.3\">𝑙</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p3.2.m2.3c\">A_{l,1}(x_{l})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p3.2.m2.3d\">italic_A start_POSTSUBSCRIPT italic_l , 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math>)\\nas the final output and evaluate the metrics based on it.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.3 </span>Analogy to Mixture-of-Experts</h3>\\n<div class=\"ltx_para\" id=\"S2.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p1.1\">Mixture-of-Experts (MoE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Shazeer et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib25\" title=\"\">2017</a>)</cite> is a prominent and well-established technique in machine learning where multiple expert networks specialize in different skill sets.\\nThe MoE approach has shown significant success across various applications due to its ability to leverage diverse model capabilities for complex problem-solving tasks.\\nOur MoA method draws inspiration from this methodology.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS3.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p2.2\">A typical MoE design consists of a stack of layers known as MoE layers.\\nEach layer comprises a set of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.1.m1.1\"><semantics id=\"S2.SS3.p2.1.m1.1a\"><mi id=\"S2.SS3.p2.1.m1.1.1\" xref=\"S2.SS3.p2.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.1.m1.1b\"><ci id=\"S2.SS3.p2.1.m1.1.1.cmml\" xref=\"S2.SS3.p2.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.1.m1.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.1.m1.1d\">italic_n</annotation></semantics></math> expert networks alongside a gating network and includes residual connections for improved gradient flow.\\nFormally, for layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.2.m2.1\"><semantics id=\"S2.SS3.p2.2.m2.1a\"><mi id=\"S2.SS3.p2.2.m2.1.1\" xref=\"S2.SS3.p2.2.m2.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.2.m2.1b\"><ci id=\"S2.SS3.p2.2.m2.1.1.cmml\" xref=\"S2.SS3.p2.2.m2.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.2.m2.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.2.m2.1d\">italic_i</annotation></semantics></math>, this design can be expressed as follows:</p>\\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A4.EGx2\">\\n<tbody id=\"S2.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\\\displaystyle y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.E2.m1.1\"><semantics id=\"S2.E2.m1.1a\"><msub id=\"S2.E2.m1.1.1\" xref=\"S2.E2.m1.1.1.cmml\"><mi id=\"S2.E2.m1.1.1.2\" xref=\"S2.E2.m1.1.1.2.cmml\">y</mi><mi id=\"S2.E2.m1.1.1.3\" xref=\"S2.E2.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.E2.m1.1b\"><apply id=\"S2.E2.m1.1.1.cmml\" xref=\"S2.E2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m1.1.1.1.cmml\" xref=\"S2.E2.m1.1.1\">subscript</csymbol><ci id=\"S2.E2.m1.1.1.2.cmml\" xref=\"S2.E2.m1.1.1.2\">𝑦</ci><ci id=\"S2.E2.m1.1.1.3.cmml\" xref=\"S2.E2.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E2.m1.1c\">\\\\displaystyle y_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.E2.m1.1d\">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>\\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\\\displaystyle=\\\\sum_{j=1}^{n}{G_{i,j}(x_{i})E_{i,j}(x_{i})}+x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.E2.m2.6\"><semantics id=\"S2.E2.m2.6a\"><mrow id=\"S2.E2.m2.6.6\" xref=\"S2.E2.m2.6.6.cmml\"><mi id=\"S2.E2.m2.6.6.4\" xref=\"S2.E2.m2.6.6.4.cmml\"></mi><mo id=\"S2.E2.m2.6.6.3\" xref=\"S2.E2.m2.6.6.3.cmml\">=</mo><mrow id=\"S2.E2.m2.6.6.2\" xref=\"S2.E2.m2.6.6.2.cmml\"><mrow id=\"S2.E2.m2.6.6.2.2\" xref=\"S2.E2.m2.6.6.2.2.cmml\"><mstyle displaystyle=\"true\" id=\"S2.E2.m2.6.6.2.2.3\" xref=\"S2.E2.m2.6.6.2.2.3.cmml\"><munderover id=\"S2.E2.m2.6.6.2.2.3a\" xref=\"S2.E2.m2.6.6.2.2.3.cmml\"><mo id=\"S2.E2.m2.6.6.2.2.3.2.2\" movablelimits=\"false\" xref=\"S2.E2.m2.6.6.2.2.3.2.2.cmml\">∑</mo><mrow id=\"S2.E2.m2.6.6.2.2.3.2.3\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.cmml\"><mi id=\"S2.E2.m2.6.6.2.2.3.2.3.2\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.2.cmml\">j</mi><mo id=\"S2.E2.m2.6.6.2.2.3.2.3.1\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.1.cmml\">=</mo><mn id=\"S2.E2.m2.6.6.2.2.3.2.3.3\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.3.cmml\">1</mn></mrow><mi id=\"S2.E2.m2.6.6.2.2.3.3\" xref=\"S2.E2.m2.6.6.2.2.3.3.cmml\">n</mi></munderover></mstyle><mrow id=\"S2.E2.m2.6.6.2.2.2\" xref=\"S2.E2.m2.6.6.2.2.2.cmml\"><msub id=\"S2.E2.m2.6.6.2.2.2.4\" xref=\"S2.E2.m2.6.6.2.2.2.4.cmml\"><mi id=\"S2.E2.m2.6.6.2.2.2.4.2\" xref=\"S2.E2.m2.6.6.2.2.2.4.2.cmml\">G</mi><mrow id=\"S2.E2.m2.2.2.2.4\" xref=\"S2.E2.m2.2.2.2.3.cmml\"><mi id=\"S2.E2.m2.1.1.1.1\" xref=\"S2.E2.m2.1.1.1.1.cmml\">i</mi><mo id=\"S2.E2.m2.2.2.2.4.1\" xref=\"S2.E2.m2.2.2.2.3.cmml\">,</mo><mi id=\"S2.E2.m2.2.2.2.2\" xref=\"S2.E2.m2.2.2.2.2.cmml\">j</mi></mrow></msub><mo id=\"S2.E2.m2.6.6.2.2.2.3\" xref=\"S2.E2.m2.6.6.2.2.2.3.cmml\">\\u2062</mo><mrow id=\"S2.E2.m2.5.5.1.1.1.1.1\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.cmml\"><mo id=\"S2.E2.m2.5.5.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S2.E2.m2.5.5.1.1.1.1.1.1\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.cmml\"><mi id=\"S2.E2.m2.5.5.1.1.1.1.1.1.2\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.2.cmml\">x</mi><mi id=\"S2.E2.m2.5.5.1.1.1.1.1.1.3\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S2.E2.m2.5.5.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.cmml\">)</mo></mrow><mo id=\"S2.E2.m2.6.6.2.2.2.3a\" xref=\"S2.E2.m2.6.6.2.2.2.3.cmml\">\\u2062</mo><msub id=\"S2.E2.m2.6.6.2.2.2.5\" xref=\"S2.E2.m2.6.6.2.2.2.5.cmml\"><mi id=\"S2.E2.m2.6.6.2.2.2.5.2\" xref=\"S2.E2.m2.6.6.2.2.2.5.2.cmml\">E</mi><mrow id=\"S2.E2.m2.4.4.2.4\" xref=\"S2.E2.m2.4.4.2.3.cmml\"><mi id=\"S2.E2.m2.3.3.1.1\" xref=\"S2.E2.m2.3.3.1.1.cmml\">i</mi><mo id=\"S2.E2.m2.4.4.2.4.1\" xref=\"S2.E2.m2.4.4.2.3.cmml\">,</mo><mi id=\"S2.E2.m2.4.4.2.2\" xref=\"S2.E2.m2.4.4.2.2.cmml\">j</mi></mrow></msub><mo id=\"S2.E2.m2.6.6.2.2.2.3b\" xref=\"S2.E2.m2.6.6.2.2.2.3.cmml\">\\u2062</mo><mrow id=\"S2.E2.m2.6.6.2.2.2.2.1\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.cmml\"><mo id=\"S2.E2.m2.6.6.2.2.2.2.1.2\" stretchy=\"false\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.cmml\">(</mo><msub id=\"S2.E2.m2.6.6.2.2.2.2.1.1\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.cmml\"><mi id=\"S2.E2.m2.6.6.2.2.2.2.1.1.2\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.2.cmml\">x</mi><mi id=\"S2.E2.m2.6.6.2.2.2.2.1.1.3\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.3.cmml\">i</mi></msub><mo id=\"S2.E2.m2.6.6.2.2.2.2.1.3\" stretchy=\"false\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S2.E2.m2.6.6.2.3\" xref=\"S2.E2.m2.6.6.2.3.cmml\">+</mo><msub id=\"S2.E2.m2.6.6.2.4\" xref=\"S2.E2.m2.6.6.2.4.cmml\"><mi id=\"S2.E2.m2.6.6.2.4.2\" xref=\"S2.E2.m2.6.6.2.4.2.cmml\">x</mi><mi id=\"S2.E2.m2.6.6.2.4.3\" xref=\"S2.E2.m2.6.6.2.4.3.cmml\">i</mi></msub></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.E2.m2.6b\"><apply id=\"S2.E2.m2.6.6.cmml\" xref=\"S2.E2.m2.6.6\"><eq id=\"S2.E2.m2.6.6.3.cmml\" xref=\"S2.E2.m2.6.6.3\"></eq><csymbol cd=\"latexml\" id=\"S2.E2.m2.6.6.4.cmml\" xref=\"S2.E2.m2.6.6.4\">absent</csymbol><apply id=\"S2.E2.m2.6.6.2.cmml\" xref=\"S2.E2.m2.6.6.2\"><plus id=\"S2.E2.m2.6.6.2.3.cmml\" xref=\"S2.E2.m2.6.6.2.3\"></plus><apply id=\"S2.E2.m2.6.6.2.2.cmml\" xref=\"S2.E2.m2.6.6.2.2\"><apply id=\"S2.E2.m2.6.6.2.2.3.cmml\" xref=\"S2.E2.m2.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.2.3.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.3\">superscript</csymbol><apply id=\"S2.E2.m2.6.6.2.2.3.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.2.3.2.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.3\">subscript</csymbol><sum id=\"S2.E2.m2.6.6.2.2.3.2.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.3.2.2\"></sum><apply id=\"S2.E2.m2.6.6.2.2.3.2.3.cmml\" xref=\"S2.E2.m2.6.6.2.2.3.2.3\"><eq id=\"S2.E2.m2.6.6.2.2.3.2.3.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.1\"></eq><ci id=\"S2.E2.m2.6.6.2.2.3.2.3.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.2\">𝑗</ci><cn id=\"S2.E2.m2.6.6.2.2.3.2.3.3.cmml\" type=\"integer\" xref=\"S2.E2.m2.6.6.2.2.3.2.3.3\">1</cn></apply></apply><ci id=\"S2.E2.m2.6.6.2.2.3.3.cmml\" xref=\"S2.E2.m2.6.6.2.2.3.3\">𝑛</ci></apply><apply id=\"S2.E2.m2.6.6.2.2.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.2\"><times id=\"S2.E2.m2.6.6.2.2.2.3.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.3\"></times><apply id=\"S2.E2.m2.6.6.2.2.2.4.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.4\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.2.2.4.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.4\">subscript</csymbol><ci id=\"S2.E2.m2.6.6.2.2.2.4.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.4.2\">𝐺</ci><list id=\"S2.E2.m2.2.2.2.3.cmml\" xref=\"S2.E2.m2.2.2.2.4\"><ci id=\"S2.E2.m2.1.1.1.1.cmml\" xref=\"S2.E2.m2.1.1.1.1\">𝑖</ci><ci id=\"S2.E2.m2.2.2.2.2.cmml\" xref=\"S2.E2.m2.2.2.2.2\">𝑗</ci></list></apply><apply id=\"S2.E2.m2.5.5.1.1.1.1.1.1.cmml\" xref=\"S2.E2.m2.5.5.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.5.5.1.1.1.1.1.1.1.cmml\" xref=\"S2.E2.m2.5.5.1.1.1.1.1\">subscript</csymbol><ci id=\"S2.E2.m2.5.5.1.1.1.1.1.1.2.cmml\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.2\">𝑥</ci><ci id=\"S2.E2.m2.5.5.1.1.1.1.1.1.3.cmml\" xref=\"S2.E2.m2.5.5.1.1.1.1.1.1.3\">𝑖</ci></apply><apply id=\"S2.E2.m2.6.6.2.2.2.5.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.5\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.2.2.5.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.5\">subscript</csymbol><ci id=\"S2.E2.m2.6.6.2.2.2.5.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.5.2\">𝐸</ci><list id=\"S2.E2.m2.4.4.2.3.cmml\" xref=\"S2.E2.m2.4.4.2.4\"><ci id=\"S2.E2.m2.3.3.1.1.cmml\" xref=\"S2.E2.m2.3.3.1.1\">𝑖</ci><ci id=\"S2.E2.m2.4.4.2.2.cmml\" xref=\"S2.E2.m2.4.4.2.2\">𝑗</ci></list></apply><apply id=\"S2.E2.m2.6.6.2.2.2.2.1.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.2.1\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.2.2.2.1.1.1.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.2.1\">subscript</csymbol><ci id=\"S2.E2.m2.6.6.2.2.2.2.1.1.2.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.2\">𝑥</ci><ci id=\"S2.E2.m2.6.6.2.2.2.2.1.1.3.cmml\" xref=\"S2.E2.m2.6.6.2.2.2.2.1.1.3\">𝑖</ci></apply></apply></apply><apply id=\"S2.E2.m2.6.6.2.4.cmml\" xref=\"S2.E2.m2.6.6.2.4\"><csymbol cd=\"ambiguous\" id=\"S2.E2.m2.6.6.2.4.1.cmml\" xref=\"S2.E2.m2.6.6.2.4\">subscript</csymbol><ci id=\"S2.E2.m2.6.6.2.4.2.cmml\" xref=\"S2.E2.m2.6.6.2.4.2\">𝑥</ci><ci id=\"S2.E2.m2.6.6.2.4.3.cmml\" xref=\"S2.E2.m2.6.6.2.4.3\">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.E2.m2.6c\">\\\\displaystyle=\\\\sum_{j=1}^{n}{G_{i,j}(x_{i})E_{i,j}(x_{i})}+x_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.E2.m2.6d\">= ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_G start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_E start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>\\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\\n</tr></tbody>\\n</table>\\n<p class=\"ltx_p\" id=\"S2.SS3.p2.6\">where <math alttext=\"G_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.3.m1.2\"><semantics id=\"S2.SS3.p2.3.m1.2a\"><msub id=\"S2.SS3.p2.3.m1.2.3\" xref=\"S2.SS3.p2.3.m1.2.3.cmml\"><mi id=\"S2.SS3.p2.3.m1.2.3.2\" xref=\"S2.SS3.p2.3.m1.2.3.2.cmml\">G</mi><mrow id=\"S2.SS3.p2.3.m1.2.2.2.4\" xref=\"S2.SS3.p2.3.m1.2.2.2.3.cmml\"><mi id=\"S2.SS3.p2.3.m1.1.1.1.1\" xref=\"S2.SS3.p2.3.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS3.p2.3.m1.2.2.2.4.1\" xref=\"S2.SS3.p2.3.m1.2.2.2.3.cmml\">,</mo><mi id=\"S2.SS3.p2.3.m1.2.2.2.2\" xref=\"S2.SS3.p2.3.m1.2.2.2.2.cmml\">j</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.3.m1.2b\"><apply id=\"S2.SS3.p2.3.m1.2.3.cmml\" xref=\"S2.SS3.p2.3.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS3.p2.3.m1.2.3.1.cmml\" xref=\"S2.SS3.p2.3.m1.2.3\">subscript</csymbol><ci id=\"S2.SS3.p2.3.m1.2.3.2.cmml\" xref=\"S2.SS3.p2.3.m1.2.3.2\">𝐺</ci><list id=\"S2.SS3.p2.3.m1.2.2.2.3.cmml\" xref=\"S2.SS3.p2.3.m1.2.2.2.4\"><ci id=\"S2.SS3.p2.3.m1.1.1.1.1.cmml\" xref=\"S2.SS3.p2.3.m1.1.1.1.1\">𝑖</ci><ci id=\"S2.SS3.p2.3.m1.2.2.2.2.cmml\" xref=\"S2.SS3.p2.3.m1.2.2.2.2\">𝑗</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.3.m1.2c\">G_{i,j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.3.m1.2d\">italic_G start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> represents the output from the gating network corresponding to expert <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.4.m2.1\"><semantics id=\"S2.SS3.p2.4.m2.1a\"><mi id=\"S2.SS3.p2.4.m2.1.1\" xref=\"S2.SS3.p2.4.m2.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.4.m2.1b\"><ci id=\"S2.SS3.p2.4.m2.1.1.cmml\" xref=\"S2.SS3.p2.4.m2.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.4.m2.1c\">j</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.4.m2.1d\">italic_j</annotation></semantics></math>, and <math alttext=\"E_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.5.m3.2\"><semantics id=\"S2.SS3.p2.5.m3.2a\"><msub id=\"S2.SS3.p2.5.m3.2.3\" xref=\"S2.SS3.p2.5.m3.2.3.cmml\"><mi id=\"S2.SS3.p2.5.m3.2.3.2\" xref=\"S2.SS3.p2.5.m3.2.3.2.cmml\">E</mi><mrow id=\"S2.SS3.p2.5.m3.2.2.2.4\" xref=\"S2.SS3.p2.5.m3.2.2.2.3.cmml\"><mi id=\"S2.SS3.p2.5.m3.1.1.1.1\" xref=\"S2.SS3.p2.5.m3.1.1.1.1.cmml\">i</mi><mo id=\"S2.SS3.p2.5.m3.2.2.2.4.1\" xref=\"S2.SS3.p2.5.m3.2.2.2.3.cmml\">,</mo><mi id=\"S2.SS3.p2.5.m3.2.2.2.2\" xref=\"S2.SS3.p2.5.m3.2.2.2.2.cmml\">j</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.5.m3.2b\"><apply id=\"S2.SS3.p2.5.m3.2.3.cmml\" xref=\"S2.SS3.p2.5.m3.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.SS3.p2.5.m3.2.3.1.cmml\" xref=\"S2.SS3.p2.5.m3.2.3\">subscript</csymbol><ci id=\"S2.SS3.p2.5.m3.2.3.2.cmml\" xref=\"S2.SS3.p2.5.m3.2.3.2\">𝐸</ci><list id=\"S2.SS3.p2.5.m3.2.2.2.3.cmml\" xref=\"S2.SS3.p2.5.m3.2.2.2.4\"><ci id=\"S2.SS3.p2.5.m3.1.1.1.1.cmml\" xref=\"S2.SS3.p2.5.m3.1.1.1.1\">𝑖</ci><ci id=\"S2.SS3.p2.5.m3.2.2.2.2.cmml\" xref=\"S2.SS3.p2.5.m3.2.2.2.2\">𝑗</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.5.m3.2c\">E_{i,j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.5.m3.2d\">italic_E start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denotes the function computed by expert network <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.6.m4.1\"><semantics id=\"S2.SS3.p2.6.m4.1a\"><mi id=\"S2.SS3.p2.6.m4.1.1\" xref=\"S2.SS3.p2.6.m4.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS3.p2.6.m4.1b\"><ci id=\"S2.SS3.p2.6.m4.1.1.cmml\" xref=\"S2.SS3.p2.6.m4.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS3.p2.6.m4.1c\">j</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS3.p2.6.m4.1d\">italic_j</annotation></semantics></math>.\\nThe leverage of multiple experts allows the model to learn different skill sets and focus on various aspects of the task at hand.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS3.p3\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p3.1\">From a high-level perspective, our proposed MoA framework extends the MoE concept to the model level by operating at the model level rather than at the activation level.\\nSpecifically, our MoA approach leverages LLMs and operates entirely through the prompt interface rather than requiring modifications to internal activations or weights.\\nThis means that instead of having specialized sub-networks within a single model like in MoE,\\nwe utilize multiple full-fledged LLMs across different layers.\\nNote that in our approach, we consolidate the roles of the gating network and expert networks using a LLM,\\nas the intrinsic capacity of LLMs allows them to effectively regularize inputs by interpreting prompts and generating coherent outputs without needing external mechanisms for coordination.\\n</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S2.SS3.p4\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p4.1\">Moreover, since this method relies solely on prompting capabilities inherent within off-the-shelf models:\\n(1) It eliminates computational overhead associated with fine-tuning;\\n(2) It provides flexibility and scalability: our method can be applied to the latest LLMs regardless of their size or architecture.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S3\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Evaluation</h2>\\n<div class=\"ltx_para\" id=\"S3.p1\">\\n<p class=\"ltx_p\" id=\"S3.p1.1\">This section presents a comprehensive evaluation of our proposed MoA.\\nOur findings show that:</p>\\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i1.p1.1\">We achieve significant improvements on AlpacaEval 2.0, MT-Bench, and FLASK benchmarks.\\nNotably, with open-source models only, our approach outperforms GPT-4o on AlpacaEval 2.0 and FLASK.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i2.p1.1\">We conduct extensive experiments to provide better understandings of the internal mechanism of MoA.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\\n<div class=\"ltx_para\" id=\"S3.I1.i3.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i3.p1.1\">Through a detailed budget analysis, several implementations of MoA can deliver performance comparable to GPT-4 Turbo while being 2<math alttext=\"\\\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i3.p1.1.m1.1\"><semantics id=\"S3.I1.i3.p1.1.m1.1a\"><mo id=\"S3.I1.i3.p1.1.m1.1.1\" xref=\"S3.I1.i3.p1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.I1.i3.p1.1.m1.1b\"><times id=\"S3.I1.i3.p1.1.m1.1.1.cmml\" xref=\"S3.I1.i3.p1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.I1.i3.p1.1.m1.1c\">\\\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.I1.i3.p1.1.m1.1d\">×</annotation></semantics></math> more cost-effective.</p>\\n</div>\\n</li>\\n</ol>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Setup</h3>\\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Benchmarks</h4>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px1.p1.1\">We mainly evaluate models on AlpacaEval 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubois et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib9\" title=\"\">2024</a>)</cite>,\\na leading benchmark for assessing the alignment of LLMs with human preferences.\\nIt contains 805 instructions representative of real use cases.\\nEach model’s response is directly compared against that of the GPT-4 (<span class=\"ltx_text ltx_font_typewriter\" id=\"S3.SS1.SSS0.Px1.p1.1.1\">gpt-4-1106-preview</span>),\\nwith a GPT-4-based evaluator determining the likelihood of preferring the evaluated model’s response.\\nTo ensure fairness, the evaluation employs length-controlled (LC) win rates, effectively neutralizing length bias.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>\\nThis metric tracks closely with human preferences, achieving a Spearman correlation of 0.98 with actual human evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubois et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib9\" title=\"\">2024</a>)</cite>.</span></span></span></p>\\n</div>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px1.p2.1\">Additionally, we also evaluate on MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib44\" title=\"\">2023</a>)</cite> and FLASK <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib40\" title=\"\">2023</a>)</cite>.\\nMT-Bench uses GPT-4 to grade and give a score to model’s answer.\\nFLASK, on the other hand, offers a more granular evaluation with 12 skill-specific scores.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Models</h4>\\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px2.p1.1\">In our study, we constructed our default MoA by using only open-source models to achieve competitive performance.\\nThe models included are:\\nQwen1.5-110B-Chat <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib1\" title=\"\">2023</a>)</cite>, Qwen1.5-72B-Chat,\\nWizardLM-8x22B <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib36\" title=\"\">2023a</a>)</cite>,\\nLLaMA-3-70B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib30\" title=\"\">2023b</a>)</cite>,\\nMixtral-8x22B-v0.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib14\" title=\"\">2024</a>)</cite>,\\ndbrx-instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(The Mosaic Research Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib28\" title=\"\">2024</a>)</cite>.\\nWe construct 3 MoA layers and use the same set of models in each MoA layer.\\nWe use Qwen1.5-110B-Chat as the aggregator in the last layer.\\nWe also developed a variant called MoA w/ GPT-4o,\\nwhich prioritizes high-quality outputs\\nby using GPT-4o as the aggregator in the final MoA layer.\\nAnother variant, MoA-Lite, emphasizes cost-effectiveness.\\nIt uses the same set of models as proposers\\nbut includes only 2 MoA layers and employs Qwen1.5-72B-Chat as the aggregator.\\nThis makes it more cost-effective than GPT-4o\\nwhile achieving a <math alttext=\"1.8\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.1.m1.1\"><semantics id=\"S3.SS1.SSS0.Px2.p1.1.m1.1a\"><mrow id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\"><mn id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.2\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\">1.8</mn><mo id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.1\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.SSS0.Px2.p1.1.m1.1b\"><apply id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.1\">percent</csymbol><cn id=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\" type=\"float\" xref=\"S3.SS1.SSS0.Px2.p1.1.m1.1.1.2\">1.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.SSS0.Px2.p1.1.m1.1c\">1.8\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.SSS0.Px2.p1.1.m1.1d\">1.8 %</annotation></semantics></math> improvement in quality on AlpacaEval 2.0.\\nWe ensure strict adherence to the licensing terms of all models utilized in this research.\\nFor open-source models, all inferences were ran through\\nTogether Inference Endpoint.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"h\" title=\"\">h</a>ttps://api.together.ai/playground/chat</span></span></span></p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Benchmark Results</h3>\\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">In this subsection, we present our evaluation results on three standard benchmarks:\\nAlpacaEval 2.0, MT-Bench, and FLASK.\\nThese benchmarks were chosen to comprehensively assess the performance of our approach and\\ncompare with the state-of-the-art LLMs.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S3.T2\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Results on AlpacaEval 2.0 and MT-Bench. For AlpacaEval 2.0, MoA and MoA-Lite correspond to the 6 proposer with 3 layers and with 2 layer respectively.\\nMoA w/ GPT-4o corresponds to using GPT-4o as the final aggregator in MoA.\\nWe ran our experiments three times and reported the average scores along with the standard deviation.\\n<sup class=\"ltx_sup\" id=\"S3.T2.8.1\">†</sup> denotes our replication of the AlpacaEval results.\\nWe ran all the MT-Bench scores ourselves to get turn-based scores.\\n</figcaption><div class=\"ltx_flex_figure\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S3.T2.st1\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">(a) </span>AlpacaEval 2.0</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.st1.7\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.8.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.st1.7.8.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.8.1.1.1\" style=\"font-size:90%;\">Model</span></th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.st1.7.8.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.8.1.2.1\" style=\"font-size:90%;\">LC win.</span></th>\\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.st1.7.8.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.8.1.3.1\" style=\"font-size:90%;\">win.</span></th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.2.2\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.st1.2.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.2.2.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA w/ GPT-4o</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.st1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.1.1.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">65.7<math alttext=\"{}_{\\\\pm\\\\text{0.7}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.1.1.1.1.m1.1\"><semantics id=\"S3.T2.st1.1.1.1.1.m1.1a\"><msub id=\"S3.T2.st1.1.1.1.1.m1.1.1\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.1.1.1.1.m1.1.1a\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.1.1.1.1.m1.1.1.1\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.1.1.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2a.cmml\">0.7</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.1.1.1.1.m1.1b\"><apply id=\"S3.T2.st1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1\"><apply id=\"S3.T2.st1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.1.1.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.1.1.1.1.m1.1.1.1.2\">0.7</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.1.1.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.7}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.1.1.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.7 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T2.st1.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.2.2.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">78.7<math alttext=\"{}_{\\\\pm\\\\text{0.2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.2.2.2.1.m1.1\"><semantics id=\"S3.T2.st1.2.2.2.1.m1.1a\"><msub id=\"S3.T2.st1.2.2.2.1.m1.1.1\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.2.2.2.1.m1.1.1a\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.2.2.2.1.m1.1.1.1\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.2.2.2.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2a.cmml\">0.2</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.2.2.2.1.m1.1b\"><apply id=\"S3.T2.st1.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1\"><apply id=\"S3.T2.st1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.2.2.2.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.2.2.2.1.m1.1.1.1.2\">0.2</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.2.2.2.1.m1.1c\">{}_{\\\\pm\\\\text{0.2}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.2.2.2.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.2 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.4.4\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.4.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.4.4.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.3.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.3.3.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">65.1<math alttext=\"{}_{\\\\pm\\\\text{0.6}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.3.3.1.1.m1.1\"><semantics id=\"S3.T2.st1.3.3.1.1.m1.1a\"><msub id=\"S3.T2.st1.3.3.1.1.m1.1.1\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.3.3.1.1.m1.1.1a\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.3.3.1.1.m1.1.1.1\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.3.3.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2a.cmml\">0.6</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.3.3.1.1.m1.1b\"><apply id=\"S3.T2.st1.3.3.1.1.m1.1.1.cmml\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1\"><apply id=\"S3.T2.st1.3.3.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.3.3.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.3.3.1.1.m1.1.1.1.2\">0.6</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.3.3.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.6}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.3.3.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.6 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.4.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.4.4.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">59.8<math alttext=\"{}_{\\\\pm\\\\text{0.3}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.4.4.2.1.m1.1\"><semantics id=\"S3.T2.st1.4.4.2.1.m1.1a\"><msub id=\"S3.T2.st1.4.4.2.1.m1.1.1\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.4.4.2.1.m1.1.1a\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.4.4.2.1.m1.1.1.1\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.4.4.2.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2a.cmml\">0.3</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.4.4.2.1.m1.1b\"><apply id=\"S3.T2.st1.4.4.2.1.m1.1.1.cmml\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1\"><apply id=\"S3.T2.st1.4.4.2.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.4.4.2.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.4.4.2.1.m1.1.1.1.2\">0.3</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.4.4.2.1.m1.1c\">{}_{\\\\pm\\\\text{0.3}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.4.4.2.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.3 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.6.6\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.6.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.6.6.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA-Lite</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.5.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.5.5.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">59.3<math alttext=\"{}_{\\\\pm\\\\text{0.2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.5.5.1.1.m1.1\"><semantics id=\"S3.T2.st1.5.5.1.1.m1.1a\"><msub id=\"S3.T2.st1.5.5.1.1.m1.1.1\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.5.5.1.1.m1.1.1a\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.5.5.1.1.m1.1.1.1\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.5.5.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2a.cmml\">0.2</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.5.5.1.1.m1.1b\"><apply id=\"S3.T2.st1.5.5.1.1.m1.1.1.cmml\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1\"><apply id=\"S3.T2.st1.5.5.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.5.5.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.5.5.1.1.m1.1.1.1.2\">0.2</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.5.5.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.2}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.5.5.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.2 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.6.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.6.6.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">57.0<math alttext=\"{}_{\\\\pm\\\\text{0.7}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st1.6.6.2.1.m1.1\"><semantics id=\"S3.T2.st1.6.6.2.1.m1.1a\"><msub id=\"S3.T2.st1.6.6.2.1.m1.1.1\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.cmml\"><mi id=\"S3.T2.st1.6.6.2.1.m1.1.1a\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st1.6.6.2.1.m1.1.1.1\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st1.6.6.2.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2a.cmml\">0.7</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st1.6.6.2.1.m1.1b\"><apply id=\"S3.T2.st1.6.6.2.1.m1.1.1.cmml\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1\"><apply id=\"S3.T2.st1.6.6.2.1.m1.1.1.1.cmml\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st1.6.6.2.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st1.6.6.2.1.m1.1.1.1.2\">0.7</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st1.6.6.2.1.m1.1c\">{}_{\\\\pm\\\\text{0.7}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st1.6.6.2.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.7 end_FLOATSUBSCRIPT</annotation></semantics></math>%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.9.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.9.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.9.1.1.1\" style=\"font-size:90%;\">GPT-4 Omni (05/13)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.9.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.9.1.2.1\" style=\"font-size:90%;\">57.5%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.9.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.9.1.3.1\" style=\"font-size:90%;\">51.3%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.10.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.10.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.10.2.1.1\" style=\"font-size:90%;\">GPT-4 Turbo (04/09)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.10.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.10.2.2.1\" style=\"font-size:90%;\">55.0%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.10.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.10.2.3.1\" style=\"font-size:90%;\">46.1%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.7\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.7.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\\n<span class=\"ltx_text\" id=\"S3.T2.st1.7.7.1.1\" style=\"font-size:90%;\">WizardLM 8x22B</span><sup class=\"ltx_sup\" id=\"S3.T2.st1.7.7.1.2\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.7.1.2.1\" style=\"font-size:90%;\">†</span></sup>\\n</th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.7.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.7.2.1\" style=\"font-size:90%;\">51.3%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.7.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.7.3.1\" style=\"font-size:90%;\">62.3%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.11.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.11.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.11.3.1.1\" style=\"font-size:90%;\">GPT-4 Preview (11/06)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.11.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.11.3.2.1\" style=\"font-size:90%;\">50.0%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.11.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.11.3.3.1\" style=\"font-size:90%;\">50.0%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.12.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.12.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.12.4.1.1\" style=\"font-size:90%;\">Qwen1.5 110B Chat</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.12.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.12.4.2.1\" style=\"font-size:90%;\">43.9%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.12.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.12.4.3.1\" style=\"font-size:90%;\">33.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.13.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.13.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.13.5.1.1\" style=\"font-size:90%;\">Qwen1.5 72B Chat</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.13.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.13.5.2.1\" style=\"font-size:90%;\">36.6%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.13.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.13.5.3.1\" style=\"font-size:90%;\">26.5%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.14.6\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.14.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.14.6.1.1\" style=\"font-size:90%;\">GPT-4 (03/14)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.14.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.14.6.2.1\" style=\"font-size:90%;\">35.3%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.14.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.14.6.3.1\" style=\"font-size:90%;\">22.1%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.15.7\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st1.7.15.7.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.15.7.1.1\" style=\"font-size:90%;\">Llama 3 70B Instruct</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st1.7.15.7.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.15.7.2.1\" style=\"font-size:90%;\">34.4%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st1.7.15.7.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.15.7.3.1\" style=\"font-size:90%;\">33.2%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st1.7.16.8\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.st1.7.16.8.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.16.8.1.1\" style=\"font-size:90%;\">Mixtral 8x22B v0.1</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.st1.7.16.8.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.16.8.2.1\" style=\"font-size:90%;\">30.9%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S3.T2.st1.7.16.8.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st1.7.16.8.3.1\" style=\"font-size:90%;\">22.2%</span></td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S3.T2.st2\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">(b) </span>MT-Bench.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.st2.3\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.4.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.st2.3.4.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.4.1.1.1\" style=\"font-size:90%;\">Model</span></th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.st2.3.4.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.4.1.2.1\" style=\"font-size:90%;\">Avg.</span></th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.st2.3.4.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.4.1.3.1\" style=\"font-size:90%;\">1st turn</span></th>\\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.st2.3.4.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.4.1.4.1\" style=\"font-size:90%;\">2nd turn</span></th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.1.1\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.st2.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.1.1.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA w/ GPT-4o</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.st2.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.1.1.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.40<math alttext=\"{}_{\\\\pm\\\\text{0.06}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st2.1.1.1.1.m1.1\"><semantics id=\"S3.T2.st2.1.1.1.1.m1.1a\"><msub id=\"S3.T2.st2.1.1.1.1.m1.1.1\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st2.1.1.1.1.m1.1.1a\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st2.1.1.1.1.m1.1.1.1\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st2.1.1.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2a.cmml\">0.06</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st2.1.1.1.1.m1.1b\"><apply id=\"S3.T2.st2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1\"><apply id=\"S3.T2.st2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st2.1.1.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st2.1.1.1.1.m1.1.1.1.2\">0.06</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st2.1.1.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.06}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st2.1.1.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.06 end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.st2.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.1.1.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.49</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T2.st2.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.1.1.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.31</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.5.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.5.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.5.1.1.1\" style=\"font-size:90%;\">GPT-4 Turbo (04/09)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.5.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.5.1.2.1\" style=\"font-size:90%;\">9.31</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.5.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.5.1.3.1\" style=\"font-size:90%;\">9.35</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.5.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.5.1.4.1\" style=\"font-size:90%;\">9.28</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.2.2\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.2.2.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.2.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.2.2.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.25<math alttext=\"{}_{\\\\pm\\\\text{0.10}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st2.2.2.1.1.m1.1\"><semantics id=\"S3.T2.st2.2.2.1.1.m1.1a\"><msub id=\"S3.T2.st2.2.2.1.1.m1.1.1\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st2.2.2.1.1.m1.1.1a\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st2.2.2.1.1.m1.1.1.1\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st2.2.2.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2a.cmml\">0.10</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st2.2.2.1.1.m1.1b\"><apply id=\"S3.T2.st2.2.2.1.1.m1.1.1.cmml\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1\"><apply id=\"S3.T2.st2.2.2.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st2.2.2.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st2.2.2.1.1.m1.1.1.1.2\">0.10</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st2.2.2.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.10}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st2.2.2.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.10 end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.2.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.2.2.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.44</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.2.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.2.2.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.07</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.6.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.6.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.6.2.1.1\" style=\"font-size:90%;\">GPT-4 Preview (11/06)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.6.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.6.2.2.1\" style=\"font-size:90%;\">9.20</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.6.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.6.2.3.1\" style=\"font-size:90%;\">9.38</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.6.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.6.2.4.1\" style=\"font-size:90%;\">9.03</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.7.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.7.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.7.3.1.1\" style=\"font-size:90%;\">GPT-4 Omni (05/13)</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.7.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.7.3.2.1\" style=\"font-size:90%;\">9.19</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.7.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.7.3.3.1\" style=\"font-size:90%;\">9.31</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.7.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.7.3.4.1\" style=\"font-size:90%;\">9.07</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.3\" style=\"background-color:#E6E6E6;\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.3.2.1\" style=\"font-size:90%;background-color:#E6E6E6;\">MoA-Lite</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.3.1.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.18<math alttext=\"{}_{\\\\pm\\\\text{0.09}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.st2.3.3.1.1.m1.1\"><semantics id=\"S3.T2.st2.3.3.1.1.m1.1a\"><msub id=\"S3.T2.st2.3.3.1.1.m1.1.1\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.st2.3.3.1.1.m1.1.1a\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.cmml\"></mi><mrow id=\"S3.T2.st2.3.3.1.1.m1.1.1.1\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1.cmml\"><mo id=\"S3.T2.st2.3.3.1.1.m1.1.1.1a\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1.cmml\">±</mo><mtext id=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2\" mathbackground=\"#E6E6E6\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2a.cmml\">0.09</mtext></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.st2.3.3.1.1.m1.1b\"><apply id=\"S3.T2.st2.3.3.1.1.m1.1.1.cmml\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1\"><apply id=\"S3.T2.st2.3.3.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T2.st2.3.3.1.1.m1.1.1.1.1.cmml\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1\">plus-or-minus</csymbol><ci id=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2a.cmml\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2\"><mtext id=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2.cmml\" mathbackground=\"#E6E6E6\" mathsize=\"70%\" xref=\"S3.T2.st2.3.3.1.1.m1.1.1.1.2\">0.09</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.st2.3.3.1.1.m1.1c\">{}_{\\\\pm\\\\text{0.09}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.st2.3.3.1.1.m1.1d\">start_FLOATSUBSCRIPT ± 0.09 end_FLOATSUBSCRIPT</annotation></semantics></math></span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.3.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">9.38</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.3.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">8.99</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.8.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.8.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.8.4.1.1\" style=\"font-size:90%;\">Qwen1.5 110B Chat</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.8.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.8.4.2.1\" style=\"font-size:90%;\">8.96</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.8.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.8.4.3.1\" style=\"font-size:90%;\">9.23</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.8.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.8.4.4.1\" style=\"font-size:90%;\">8.63</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.9.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.9.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.9.5.1.1\" style=\"font-size:90%;\">Llama 3 70B Instruct</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.9.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.9.5.2.1\" style=\"font-size:90%;\">8.94</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.9.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.9.5.3.1\" style=\"font-size:90%;\">9.2</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.9.5.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.9.5.4.1\" style=\"font-size:90%;\">8.68</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.10.6\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.10.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.10.6.1.1\" style=\"font-size:90%;\">Mixtral 8x22B v0.1</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.10.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.10.6.2.1\" style=\"font-size:90%;\">8.78</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.10.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.10.6.3.1\" style=\"font-size:90%;\">9.11</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.10.6.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.10.6.4.1\" style=\"font-size:90%;\">8.44</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.11.7\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.11.7.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.11.7.1.1\" style=\"font-size:90%;\">WizardLM 8x22B</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.11.7.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.11.7.2.1\" style=\"font-size:90%;\">8.78</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.11.7.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.11.7.3.1\" style=\"font-size:90%;\">8.96</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.11.7.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.11.7.4.1\" style=\"font-size:90%;\">8.61</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.12.8\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.st2.3.12.8.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.12.8.1.1\" style=\"font-size:90%;\">Qwen1.5 72B Chat</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.12.8.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.12.8.2.1\" style=\"font-size:90%;\">8.44</span></td>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.st2.3.12.8.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.12.8.3.1\" style=\"font-size:90%;\">8.55</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.st2.3.12.8.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.12.8.4.1\" style=\"font-size:90%;\">8.34</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.st2.3.13.9\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.st2.3.13.9.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.13.9.1.1\" style=\"font-size:90%;\">GPT-4 (06/13)</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.st2.3.13.9.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.13.9.2.1\" style=\"font-size:90%;\">8.84</span></td>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.st2.3.13.9.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.13.9.3.1\" style=\"font-size:90%;\">9.08</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S3.T2.st2.3.13.9.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T2.st2.3.13.9.4.1\" style=\"font-size:90%;\">8.61</span></td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">AlpacaEval 2.0</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px1.p1.7\">We conducted comparisons against leading models such as GPT-4 and other state-of-the-art open-source models.\\nThe detailed results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.T2.st1\" title=\"In Table 2 ‣ 3.2 Benchmark Results ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> where our MoA methodology achieved top positions on the AlpacaEval 2.0 leaderboard,\\ndemonstrating a remarkable <math alttext=\"8.2\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.1.m1.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.1.m1.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml\">8.2</mn><mo id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.1.m1.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.1.m1.1.1.2\">8.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.1.m1.1c\">8.2\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.1.m1.1d\">8.2 %</annotation></semantics></math> absolute improvement over the previous top model, GPT-4o.\\nMoreover, it is particularly noteworthy that our model outperformed GPT-4o using solely open-source models,\\nachieving a margin of <math alttext=\"7.6\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.2.m2.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.2.m2.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml\">7.6</mn><mo id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.2.m2.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.2.m2.1.1.2\">7.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.2.m2.1c\">7.6\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.2.m2.1d\">7.6 %</annotation></semantics></math> absolute improvement from <math alttext=\"57.5\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.3.m3.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.3.m3.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml\">57.5</mn><mo id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.3.m3.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.3.m3.1.1.2\">57.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.3.m3.1c\">57.5\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.3.m3.1d\">57.5 %</annotation></semantics></math> (GPT-4o) to <math alttext=\"65.1\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.4.m4.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.4.m4.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml\">65.1</mn><mo id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.4.m4.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.4.m4.1.1.2\">65.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.4.m4.1c\">65.1\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.4.m4.1d\">65.1 %</annotation></semantics></math> (MoA).\\nOur MoA-Lite setup uses less layers and being more cost-effective.\\nEven with this lighter approach, we still outperform the best model by <math alttext=\"1.8\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.5.m5.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.5.m5.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml\">1.8</mn><mo id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.5.m5.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.5.m5.1.1.2\">1.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.5.m5.1c\">1.8\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.5.m5.1d\">1.8 %</annotation></semantics></math>, improving from <math alttext=\"57.5\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.6.m6.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.6.m6.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml\">57.5</mn><mo id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.6.m6.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.6.m6.1.1.2\">57.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.6.m6.1c\">57.5\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.6.m6.1d\">57.5 %</annotation></semantics></math> (GPT-4o) to <math alttext=\"59.3\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.7.m7.1\"><semantics id=\"S3.SS2.SSS0.Px1.p1.7.m7.1a\"><mrow id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml\"><mn id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.2\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.2.cmml\">59.3</mn><mo id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.1\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.SSS0.Px1.p1.7.m7.1b\"><apply id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.1.cmml\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.1\">percent</csymbol><cn id=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.2.cmml\" type=\"float\" xref=\"S3.SS2.SSS0.Px1.p1.7.m7.1.1.2\">59.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.SSS0.Px1.p1.7.m7.1c\">59.3\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS2.SSS0.Px1.p1.7.m7.1d\">59.3 %</annotation></semantics></math> (MoA-Lite).\\nThis further highlights the effectiveness of our method\\nin leveraging open-source models capabilities with varying compute budget to their fullest potential.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"526\" id=\"S3.F3.g1\" src=\"x3.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F3.2.1.1\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" id=\"S3.F3.3.2\" style=\"font-size:90%;\">\\nResults on FLASK where we use the 6 proposer MoA setup and Qwen1.5-110B-Chat is the aggregator.\\n</span></figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">MT-Bench</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px2.p1.1\">Though improvements over individual models on the MT-Bench are relatively incremental,\\nthis is understandable given that current models already perform exceptionally well on this benchmark,\\nas a single model alone can achieve scores greater than 9 out of 10.\\nDespite the marginal enhancements, our approach still secures the top position on the leaderboard.\\nThis demonstrates that even with already highly optimized benchmarks,\\nour method can push the boundaries further, maintaining the leadership.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px3\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">FLASK</h4>\\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px3.p1.1\">FLASK provides fine-grained evaluation of models.\\nAmong those metrics, MoA excels in several key aspects.\\nSpecifically, our methodology shows significant improvement in\\nrobustness, correctness, efficiency, factuality, commonsense,\\ninsightfulness, completeness,\\ncompared to the single model score of the aggregator, Qwen-110B-Chat.\\nAdditionally, MoA also outperforms GPT-4 Omni in terms of correctness, factuality, insightfulness, completeness, and metacognition.\\nOne metric where MoA did not do as well was conciseness;\\nthe model produced outputs that were marginally more verbose.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>What Makes Mixture-of-Agents Work Well?</h3>\\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">In this subsection, we conduct experiments that provide us better understandings of the internal mechanism of Mixture-of-Agents. We summarize key insights below.</p>\\n</div>\\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Mixture-of-Agents significantly outperforms LLM rankers.</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px1.p1.1\">First, we compare Mixture-of-Agents with an LLM-based ranker which uses the aggregator model to select one of the answers that are generated by the proposers, instead of generating a new output. The results are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F4\" title=\"In MoA tends to incorporate the best proposed answers. ‣ 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a>, where we can observe that the MoA approach significantly outperforms an LLM-ranker baseline. The fact that MoA outperforms the ranking approach suggests that the aggregator does not simply select one of the generated answers by the proposers, but potentially performs sophisticated aggregation over all proposed generations.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">MoA tends to incorporate the best proposed answers.</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px2.p1.3\">We also compare the aggregator’s response with the proposers’ responses via similarity scores such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib22\" title=\"\">2002</a>)</cite> which reflects n-gram overlaps. Within each sample, given <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1\"><semantics id=\"S3.SS3.SSS0.Px2.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1d\">italic_n</annotation></semantics></math> proposed answers by the proposers, we calculate the the Spearman’s rank correlation coefficient between <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1\"><semantics id=\"S3.SS3.SSS0.Px2.p1.2.m2.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1d\">italic_n</annotation></semantics></math> similar scores and <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1\"><semantics id=\"S3.SS3.SSS0.Px2.p1.3.m3.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1d\">italic_n</annotation></semantics></math> preference scores determined by the GPT-4 based evaluator.\\nThe results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F4\" title=\"In MoA tends to incorporate the best proposed answers. ‣ 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> indeed confirms a positive correlation between the win rate and the BLEU score.\\nWe also provide results with Levenshtein similarity <cite class=\"ltx_cite ltx_citemacro_citep\">(RapidFuzz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib23\" title=\"\">2023</a>)</cite> or TF-IDF as opposed to BLEU scores in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A1\" title=\"Appendix A Spearman Correlation using Different Similarity Functions ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\\nwhere both alternative approaches for textual similarities also yield positive correlation with the preference scores.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S3.F4\">\\n<div class=\"ltx_flex_figure\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S3.F4.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"272\" id=\"S3.F4.1.g1\" src=\"x4.png\" width=\"343\"/>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S3.F4.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"272\" id=\"S3.F4.2.g1\" src=\"x5.png\" width=\"367\"/>\\n</figure>\\n</div>\\n</div>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F4.4.1.1\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" id=\"S3.F4.5.2\" style=\"font-size:90%;\">\\n(a) LC win rate on AlpacaEval 2.0 with different aggregators in the 6-model Mixture-of-Agents setup. All the curves use the same 6 proposer agents; they only differ in the choice of the final aggregator. The LLM ranker uses Qwen1.5-110B-Chat model with a prompt format in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A2.T5\" title=\"In Appendix B LLM Ranker ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The GPT-4o model is only used to aggregate the output for the purpose of evaluation and does not participate as a proposer towards the next layer.\\n(b) Spearman correlation between BLEU scores (calculated using 3-gram, 4-gram, and 5-gram metrics) and win rate of the proposed outputs. </span></figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px3\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Effect of model diversity and the number of proposers.</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px3.p1.4\">We analyze how the number of proposals affect the final output quality by varying <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.1\"><semantics id=\"S3.SS3.SSS0.Px3.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.1d\">italic_n</annotation></semantics></math>, the number of proposers in each layer.\\nWe show the results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.T4\" title=\"In Specialization of models in the Mixture-of-Agent ecosystem. ‣ 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> where we find that scores increases monotonically with <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1\"><semantics id=\"S3.SS3.SSS0.Px3.p1.2.m2.1a\"><mi id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1b\"><ci id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1d\">italic_n</annotation></semantics></math>, reflecting the benefits of having more auxiliary information. In addition, we also quantify the impact of using a diverse set of LLMs as proposers. For each <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1\"><semantics id=\"S3.SS3.SSS0.Px3.p1.3.m3.1a\"><mi id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1b\"><ci id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1d\">italic_n</annotation></semantics></math>, we compare two settings: “single-proposer” where the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px3.p1.4.m4.1\"><semantics id=\"S3.SS3.SSS0.Px3.p1.4.m4.1a\"><mi id=\"S3.SS3.SSS0.Px3.p1.4.m4.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.4.m4.1b\"><ci id=\"S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.4.m4.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.4.m4.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px3.p1.4.m4.1d\">italic_n</annotation></semantics></math> responses are generated by the same LLM with\\na temperature of 0.7;\\nand “multiple-proposer” where each response is generated by a different LLMs. Overall, using multiple different LLMs consistently yielded better results. Both results suggest that having a larger number of diverse LLM agents in each MoA layer can improve performance. Further scaling the width of MoA is a promising direction of future investigation.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px4\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Specialization of models in the Mixture-of-Agent ecosystem.</h4>\\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px4.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px4.p1.1\">We also conducted experiments to determine which models excel in specific roles.\\nSpecifically, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.T4\" title=\"In Specialization of models in the Mixture-of-Agent ecosystem. ‣ 3.3 What Makes Mixture-of-Agents Work Well? ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that GPT-4o, Qwen, LLaMA-3 emerged as a versatile model effective in both assisting and aggregating tasks.\\nIn contrast, WizardLM demonstrated excellent performance as an proposer model\\nbut struggled to maintain its effectiveness in aggregating responses from other models.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S3.T4\">\\n<div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_top\" id=\"S3.T4.6\" style=\"width:212.5pt;\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_figure\">Table 3: </span>Effects of the number of proposer models on AlpacaEval 2.0.\\nWe denote <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.2.2.m1.1\"><semantics id=\"S3.T4.2.2.m1.1b\"><mi id=\"S3.T4.2.2.m1.1.1\" xref=\"S3.T4.2.2.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.2.2.m1.1c\"><ci id=\"S3.T4.2.2.m1.1.1.cmml\" xref=\"S3.T4.2.2.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.2.2.m1.1d\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T4.2.2.m1.1e\">italic_n</annotation></semantics></math> as either the number of agents in an MoA layer or the number of proposed outputs in the single-proposer setting.\\nWe use Qwen1.5-110B-Chat as the aggregator and use 2 MoA layers for all settings in this table.\\n</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.6.6\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.6.6.5.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T4.6.6.5.1.1\"><span class=\"ltx_text\" id=\"S3.T4.6.6.5.1.1.1\" style=\"font-size:90%;\">Setting</span></th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.6.6.5.1.2\"><span class=\"ltx_text\" id=\"S3.T4.6.6.5.1.2.1\" style=\"font-size:90%;\">Multiple-Proposer</span></th>\\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.6.6.5.1.3\"><span class=\"ltx_text\" id=\"S3.T4.6.6.5.1.3.1\" style=\"font-size:90%;\">Single-Proposer</span></th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.3.3.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.3.3.1.1\"><math alttext=\"n=6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.3.3.1.1.m1.1\"><semantics id=\"S3.T4.3.3.1.1.m1.1a\"><mrow id=\"S3.T4.3.3.1.1.m1.1.1\" xref=\"S3.T4.3.3.1.1.m1.1.1.cmml\"><mi id=\"S3.T4.3.3.1.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S3.T4.3.3.1.1.m1.1.1.2.cmml\">n</mi><mo id=\"S3.T4.3.3.1.1.m1.1.1.1\" mathsize=\"90%\" xref=\"S3.T4.3.3.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.T4.3.3.1.1.m1.1.1.3\" mathsize=\"90%\" xref=\"S3.T4.3.3.1.1.m1.1.1.3.cmml\">6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.3.3.1.1.m1.1b\"><apply id=\"S3.T4.3.3.1.1.m1.1.1.cmml\" xref=\"S3.T4.3.3.1.1.m1.1.1\"><eq id=\"S3.T4.3.3.1.1.m1.1.1.1.cmml\" xref=\"S3.T4.3.3.1.1.m1.1.1.1\"></eq><ci id=\"S3.T4.3.3.1.1.m1.1.1.2.cmml\" xref=\"S3.T4.3.3.1.1.m1.1.1.2\">𝑛</ci><cn id=\"S3.T4.3.3.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T4.3.3.1.1.m1.1.1.3\">6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.3.3.1.1.m1.1c\">n=6</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T4.3.3.1.1.m1.1d\">italic_n = 6</annotation></semantics></math></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.3.3.1.2\"><span class=\"ltx_text\" id=\"S3.T4.3.3.1.2.1\" style=\"font-size:90%;\">61.3%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T4.3.3.1.3\"><span class=\"ltx_text\" id=\"S3.T4.3.3.1.3.1\" style=\"font-size:90%;\">56.7%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.4.4.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.4.4.2.1\"><math alttext=\"n=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.4.4.2.1.m1.1\"><semantics id=\"S3.T4.4.4.2.1.m1.1a\"><mrow id=\"S3.T4.4.4.2.1.m1.1.1\" xref=\"S3.T4.4.4.2.1.m1.1.1.cmml\"><mi id=\"S3.T4.4.4.2.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S3.T4.4.4.2.1.m1.1.1.2.cmml\">n</mi><mo id=\"S3.T4.4.4.2.1.m1.1.1.1\" mathsize=\"90%\" xref=\"S3.T4.4.4.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.T4.4.4.2.1.m1.1.1.3\" mathsize=\"90%\" xref=\"S3.T4.4.4.2.1.m1.1.1.3.cmml\">3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.4.4.2.1.m1.1b\"><apply id=\"S3.T4.4.4.2.1.m1.1.1.cmml\" xref=\"S3.T4.4.4.2.1.m1.1.1\"><eq id=\"S3.T4.4.4.2.1.m1.1.1.1.cmml\" xref=\"S3.T4.4.4.2.1.m1.1.1.1\"></eq><ci id=\"S3.T4.4.4.2.1.m1.1.1.2.cmml\" xref=\"S3.T4.4.4.2.1.m1.1.1.2\">𝑛</ci><cn id=\"S3.T4.4.4.2.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T4.4.4.2.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.4.4.2.1.m1.1c\">n=3</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T4.4.4.2.1.m1.1d\">italic_n = 3</annotation></semantics></math></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.4.4.2.2\"><span class=\"ltx_text\" id=\"S3.T4.4.4.2.2.1\" style=\"font-size:90%;\">58.0%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.4.4.2.3\"><span class=\"ltx_text\" id=\"S3.T4.4.4.2.3.1\" style=\"font-size:90%;\">56.1%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.5.5.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.5.5.3.1\"><math alttext=\"n=2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.5.5.3.1.m1.1\"><semantics id=\"S3.T4.5.5.3.1.m1.1a\"><mrow id=\"S3.T4.5.5.3.1.m1.1.1\" xref=\"S3.T4.5.5.3.1.m1.1.1.cmml\"><mi id=\"S3.T4.5.5.3.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S3.T4.5.5.3.1.m1.1.1.2.cmml\">n</mi><mo id=\"S3.T4.5.5.3.1.m1.1.1.1\" mathsize=\"90%\" xref=\"S3.T4.5.5.3.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.T4.5.5.3.1.m1.1.1.3\" mathsize=\"90%\" xref=\"S3.T4.5.5.3.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.5.5.3.1.m1.1b\"><apply id=\"S3.T4.5.5.3.1.m1.1.1.cmml\" xref=\"S3.T4.5.5.3.1.m1.1.1\"><eq id=\"S3.T4.5.5.3.1.m1.1.1.1.cmml\" xref=\"S3.T4.5.5.3.1.m1.1.1.1\"></eq><ci id=\"S3.T4.5.5.3.1.m1.1.1.2.cmml\" xref=\"S3.T4.5.5.3.1.m1.1.1.2\">𝑛</ci><cn id=\"S3.T4.5.5.3.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T4.5.5.3.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.5.5.3.1.m1.1c\">n=2</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T4.5.5.3.1.m1.1d\">italic_n = 2</annotation></semantics></math></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.5.5.3.2\"><span class=\"ltx_text\" id=\"S3.T4.5.5.3.2.1\" style=\"font-size:90%;\">58.8%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.5.5.3.3\"><span class=\"ltx_text\" id=\"S3.T4.5.5.3.3.1\" style=\"font-size:90%;\">54.5%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.6.6.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.6.6.4.1\"><math alttext=\"n=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.6.6.4.1.m1.1\"><semantics id=\"S3.T4.6.6.4.1.m1.1a\"><mrow id=\"S3.T4.6.6.4.1.m1.1.1\" xref=\"S3.T4.6.6.4.1.m1.1.1.cmml\"><mi id=\"S3.T4.6.6.4.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S3.T4.6.6.4.1.m1.1.1.2.cmml\">n</mi><mo id=\"S3.T4.6.6.4.1.m1.1.1.1\" mathsize=\"90%\" xref=\"S3.T4.6.6.4.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.T4.6.6.4.1.m1.1.1.3\" mathsize=\"90%\" xref=\"S3.T4.6.6.4.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.6.6.4.1.m1.1b\"><apply id=\"S3.T4.6.6.4.1.m1.1.1.cmml\" xref=\"S3.T4.6.6.4.1.m1.1.1\"><eq id=\"S3.T4.6.6.4.1.m1.1.1.1.cmml\" xref=\"S3.T4.6.6.4.1.m1.1.1.1\"></eq><ci id=\"S3.T4.6.6.4.1.m1.1.1.2.cmml\" xref=\"S3.T4.6.6.4.1.m1.1.1.2\">𝑛</ci><cn id=\"S3.T4.6.6.4.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T4.6.6.4.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.6.6.4.1.m1.1c\">n=1</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T4.6.6.4.1.m1.1d\">italic_n = 1</annotation></semantics></math></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.6.6.4.2\"><span class=\"ltx_text\" id=\"S3.T4.6.6.4.2.1\" style=\"font-size:90%;\">47.8%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S3.T4.6.6.4.3\"><span class=\"ltx_text\" id=\"S3.T4.6.6.4.3.1\" style=\"font-size:90%;\">47.8%</span></td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_top\" id=\"S3.T4.fig1\" style=\"width:212.5pt;\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_figure\">Table 4: </span>\\nImpact of different models serving as proposers vs aggregators.\\nWhen evaluating different aggregators, all six models serve as proposers;\\nwhen evaluating proposers, Qwen1.5-110B-Chat serves as the aggregator.\\nWe use 2 MoA layers in this table.\\n</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.fig1.3\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T4.fig1.3.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.1.1.1.1\" style=\"font-size:90%;\">Model</span></th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.fig1.3.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.1.1.2.1\" style=\"font-size:90%;\">As aggregator</span></th>\\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T4.fig1.3.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.1.1.3.1\" style=\"font-size:90%;\">As proposer</span></th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.2.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.fig1.3.2.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.2.1.1.1\" style=\"font-size:90%;\">Qwen1.5-110B-Chat</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.fig1.3.2.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.2.1.2.1\" style=\"font-size:90%;\">61.3%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T4.fig1.3.2.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.2.1.3.1\" style=\"font-size:90%;\">56.7%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.3.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.fig1.3.3.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.3.2.1.1\" style=\"font-size:90%;\">Qwen1.5-72B-Chat</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.fig1.3.3.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.3.2.2.1\" style=\"font-size:90%;\">59.3%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.fig1.3.3.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.3.2.3.1\" style=\"font-size:90%;\">53.3%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.4.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.fig1.3.4.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.4.3.1.1\" style=\"font-size:90%;\">LLaMA-3-70b-Instruct</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.fig1.3.4.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.4.3.2.1\" style=\"font-size:90%;\">45.0%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.fig1.3.4.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.4.3.3.1\" style=\"font-size:90%;\">60.6%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.5.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.fig1.3.5.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.5.4.1.1\" style=\"font-size:90%;\">WizardLM 8x22B</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.fig1.3.5.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.5.4.2.1\" style=\"font-size:90%;\">52.9%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.fig1.3.5.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.5.4.3.1\" style=\"font-size:90%;\">63.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.6.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.fig1.3.6.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.6.5.1.1\" style=\"font-size:90%;\">Mixtral-8x22B-Instruct</span></th>\\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.fig1.3.6.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.6.5.2.1\" style=\"font-size:90%;\">48.4%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T4.fig1.3.6.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.6.5.3.1\" style=\"font-size:90%;\">54.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.fig1.3.7.6\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.fig1.3.7.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.7.6.1.1\" style=\"font-size:90%;\">dbrx-instruct</span></th>\\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.fig1.3.7.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.7.6.2.1\" style=\"font-size:90%;\">41.5%</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S3.T4.fig1.3.7.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.fig1.3.7.6.3.1\" style=\"font-size:90%;\">55.1%</span></td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n</section>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>Budget and Token Analysis</h3>\\n<figure class=\"ltx_figure\" id=\"S3.F5\">\\n<div class=\"ltx_flex_figure\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S3.F5.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"622\" id=\"S3.F5.sf1.g1\" src=\"x6.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F5.sf1.2.1.1\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" id=\"S3.F5.sf1.3.2\" style=\"font-size:90%;\">LC win rate vs. cost</span></figcaption>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S3.F5.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"622\" id=\"S3.F5.sf2.g1\" src=\"x7.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F5.sf2.2.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"S3.F5.sf2.3.2\" style=\"font-size:90%;\">LC win rate vs. tflops</span></figcaption>\\n</figure>\\n</div>\\n</div>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F5.4.1.1\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" id=\"S3.F5.5.2\" style=\"font-size:90%;\">(a) Performance trade-off versus cost. (b) Performance trade-off versus the number of tera floating operations (tflops), which we use as a proxy for latency. Note that we calculate the sum over layers of the max number of tflops among proposers in each MoA layer as multiple proposers can run in parallel.\\nOur plots illustrate a Pareto frontier where we can choose a model progressively higher score with the lowest cost for such level of performance.\\nWe show that the Mixture-of-Agents approach lie on this Pareto front,\\nas opposed to GPT-4 Turbo and GPT-4o which are not cost optimal and is more expensive compared to MoA approaches of the same LC win rate.\\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.F5.5.2.1\">Single Proposer</span>: uses the same model to generate multiple responses in each MoA layer;\\n<span class=\"ltx_text ltx_font_italic\" id=\"S3.F5.5.2.2\">Multi Proposer</span>: uses different models in each MoA layer. The actual tflops of GPT-4 is unknown, so we use the rumored size from the community of an 8x220B architecture.\\n</span></figcaption>\\n</figure>\\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p1.1\">To understand the relationship between budget, token usage, and LC win rates,\\nwe conducted a budget and token analysis.\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F5.sf1\" title=\"In Figure 5 ‣ 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5(a)</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F5.sf2\" title=\"In Figure 5 ‣ 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5(b)</span></a> illustrate these relationships.</p>\\n</div>\\n<section class=\"ltx_paragraph\" id=\"S3.SS4.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Cost Effectiveness</h4>\\n<div class=\"ltx_para\" id=\"S3.SS4.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.SSS0.Px1.p1.1\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F5.sf1\" title=\"In Figure 5 ‣ 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5(a)</span></a>, we plot the LC win rate against\\nthe average inference cost for each instance in the AplacaEval 2.0 benchmark.\\nThe cost is calculated based on pricing information available from API provider websites.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>\\nFor open-source models, we calculate the price using data from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://api.together.ai/models\" title=\"\">https://api.together.ai/models</a>;\\nfor OpenAI models, we use pricing details from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/api/pricing/\" title=\"\">https://openai.com/api/pricing/</a>.\\nPricing data was retrieved as of May 22, 2024.\\n</span></span></span>\\nThis helps identify cost-effective models that achieve high performance without incurring excessive expenses.\\nThe chart reveals a Pareto front where certain models strike an optimal balance between cost and performance.\\nModels closer to this Pareto front are more desirable as they provide better monetary value by delivering high LC win rates at lower costs.\\nSpecifically, if we prioritize the quality, MoA is the best configuration.\\nHowever, if we want to strike a good balance between quality and cost, MoA-Lite can match GPT-4o’s cost while achieving higher level of quality.\\nNotably, it outperforms GPT-4 Turbo by approximately <math alttext=\"4\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1\"><semantics id=\"S3.SS4.SSS0.Px1.p1.1.m1.1a\"><mrow id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\"><mn id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.2\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml\">4</mn><mo id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1b\"><apply id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.1\">percent</csymbol><cn id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.2\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1c\">4\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1d\">4 %</annotation></semantics></math> while being more than twice as cost-effective.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S3.SS4.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Tflops Consumption</h4>\\n<div class=\"ltx_para\" id=\"S3.SS4.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.SSS0.Px2.p1.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#S3.F5.sf2\" title=\"In Figure 5 ‣ 3.4 Budget and Token Analysis ‣ 3 Evaluation ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5(b)</span></a> depicts the relationship between LC win rate and the number of tflops.\\nHere we use the number of tflops as a proxy for latency since latency can vary depending on the inference systems.\\nThis analysis is crucial for understanding how different models manage their budgets\\nwhile maintaining or improving performance levels.\\nSimilar to the cost efficiency analysis, a Pareto front can be observed here as well.\\nModels on this front effectively utilize their computational resource to maximize their LC win rate.</p>\\n</div>\\n</section>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S4\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Related Work</h2>\\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>LLM Reasoning</h3>\\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS1.p1.1\">In order to improve generation quality of LLMs,\\nrecent researches have experienced great progresses in optimizing LLMs to various downstream tasks through prompt engineering.\\nChain of Thought (CoT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib35\" title=\"\">2022</a>; Kojima et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib16\" title=\"\">2022</a>)</cite> prompting techniques represent a linear problem-solving approach where each step builds upon the previous one.\\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fu et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib10\" title=\"\">2022</a>)</cite> applied CoT to multi-step reasoning tasks.\\nTo automate CoT prompting, Auto-CoT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib43\" title=\"\">2022b</a>)</cite> constructs demonstrations by sampling diverse questions and generating reasoning chains.\\nActive-Prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Diao et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib7\" title=\"\">2023</a>)</cite> focuses on selecting the most uncertain questions for task-specific annotations.\\nPS Prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib32\" title=\"\">2023</a>)</cite> decomposes tasks into subtasks.\\nTree-of-Thought (ToT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib38\" title=\"\">2023a</a>)</cite> expands on the reasoning process by considering multiple paths of reasoning and self-evaluating choices.\\nEffective Graph-of-Thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib39\" title=\"\">2023b</a>)</cite> frames thoughts as graphs.\\nNatural Program prompting <cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib18\" title=\"\">2023</a>)</cite> is proposed for better solving deductive reasoning tasks.\\nAnd re-reading prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib37\" title=\"\">2023b</a>)</cite> revisits question information embedded within input prompts.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Model Ensemble</h3>\\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p1.1\">A straightforward solution to leverage the strengths of multiple models is reranking outputs from different models.\\nFor instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib15\" title=\"\">2023</a>)</cite> introduce <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.SS2.p1.1.1\">PairRanker</span>,\\nwhich performs pairwise comparisons on candidate outputs to select the best one, showing improvements on a self-constructed instruction dataset.\\nTo address the substantial computational costs associated with multi-LLM inference,\\nother studies have explored training a <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS2.p1.1.2\">router</span> that predicts the best-performing model from a fixed set of LLMs for a given input\\xa0<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib31\" title=\"\">2024a</a>; Shnitzer et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib26\" title=\"\">2024</a>; Lu et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib19\" title=\"\">2023</a>)</cite>.\\nAdditionally, FrugalGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib5\" title=\"\">2023b</a>)</cite> proposed reducing the cost of using LLMs by employing different models in a cascading manner.\\nIn order to better leverage the responses of multiple models,\\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib15\" title=\"\">2023</a>)</cite> trained a <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.SS2.p1.1.3\">GenFuser</span>,\\na model that was trained to generate an improved response to capitalize on the strengths of multiple candidates.\\n<cite class=\"ltx_cite ltx_citemacro_citet\">Huang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to fuse the outputs of different models by averaging their output probability distributions.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p2.1\">Another line of work is multi-agent collaboration.\\nSeveral studies explore using multiple large language models as agents that collectively discuss and reason through given problems interactively.\\n<cite class=\"ltx_cite ltx_citemacro_citet\">Du et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib8\" title=\"\">2023</a>)</cite> establishes a mechanism for symmetric discussions among agents.\\nAround the same time, MAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib17\" title=\"\">2023</a>)</cite> introduces an asymmetric mechanism design, with different roles, i.e., debater and judge. Other similar works include <cite class=\"ltx_cite ltx_citemacro_citep\">(Chan et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib3\" title=\"\">2023</a>)</cite>.\\nMoreover, ReConcile <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et\\xa0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib4\" title=\"\">2023a</a>)</cite> exemplifies an asymmetric discussion involving weighted voting.\\nTo understand discussion more deeply, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib41\" title=\"\">2023</a>)</cite> aim to explain such collaboration mechanism in a social psychology view.\\n<cite class=\"ltx_cite ltx_citemacro_citet\">Wang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib33\" title=\"\">2024b</a>)</cite> systematically compared multi-agent approaches and found a single agent with a strong prompt including detailed demonstrations can achieve comparable response quality to multi-agent approaches.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S5\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion</h2>\\n<div class=\"ltx_para\" id=\"S5.p1\">\\n<p class=\"ltx_p\" id=\"S5.p1.1\">This paper introduces a Mixture-of-Agents approach aimed at leveraging the capabilities of multiple LLMs via successive stages for iterative collaboration. Our method harnesses the collective strengths of agents in the Mixture-of-Agents family, and can significantly improve upon the output quality of each individual model.\\nEmpirical evaluations conducted on AlpacaEval 2.0, MT-Bench, and FLASK demonstrated substantial improvements in response quality, with our approach achieving the LC win rate up to <math alttext=\"65\\\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.1.m1.1\"><semantics id=\"S5.p1.1.m1.1a\"><mrow id=\"S5.p1.1.m1.1.1\" xref=\"S5.p1.1.m1.1.1.cmml\"><mn id=\"S5.p1.1.m1.1.1.2\" xref=\"S5.p1.1.m1.1.1.2.cmml\">65</mn><mo id=\"S5.p1.1.m1.1.1.1\" xref=\"S5.p1.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.p1.1.m1.1b\"><apply id=\"S5.p1.1.m1.1.1.cmml\" xref=\"S5.p1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.p1.1.m1.1.1.1.cmml\" xref=\"S5.p1.1.m1.1.1.1\">percent</csymbol><cn id=\"S5.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.p1.1.m1.1.1.2\">65</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.p1.1.m1.1c\">65\\\\%</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.p1.1.m1.1d\">65 %</annotation></semantics></math>.\\nThese findings validate our hypothesis that integrating diverse perspectives from various models can lead to superior performance compared to relying on a single model alone. In addition, we provide insights into improving the design of MoA; systematic optimization of MoA architecture is an interesting direction for future work.</p>\\n</div>\\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations.</h4>\\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px1.p1.1\">Our proposed method requires iterative aggregation of model responses,\\nwhich means the model cannot decide the first token until the last MoA layer is reached.\\nThis potentially results in a high Time to First Token (TTFT), which can negatively impact user experience.\\nTo mitigate this issue, we can limit the number of MoA layers,\\nas the first response aggregation has the most significant boost on generation quality.\\nFuture work could explore chunk-wise aggregation instead of aggregating entire responses at once, which can reduce TTFT while maintaining response quality.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Broader Impact.</h4>\\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px2.p1.1\">This study holds the potential to enhance the effectiveness of LLM-driven chat assistants, thereby making AI more accessible.\\nMoreover, since the intermediate outputs that are expressed in natural language, MoA presented improves the interpretability of models.\\nThis enhanced interpretability facilitates better alignment with human reasoning.</p>\\n</div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n</section>\\n<section class=\"ltx_bibliography\" id=\"bib\">\\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\\n<ul class=\"ltx_biblist\">\\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bai et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nBai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Qwen technical report.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib1.1.1\">arXiv preprint arXiv:2309.16609</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown et\\xa0al. (2020)</span>\\n<span class=\"ltx_bibblock\">\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.\\xa0D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Language models are few-shot learners.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.1.1\">Advances in neural information processing systems</em>, 33:1877–1901, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chan et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nChan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Chateval: Towards better llm-based evaluators through multi-agent debate.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib3.1.1\">arXiv preprint arXiv:2308.07201</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et\\xa0al. (2023a)</span>\\n<span class=\"ltx_bibblock\">\\nChen, J. C.-Y., Saha, S., and Bansal, M.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Reconcile: Round-table conference improves reasoning via consensus among diverse llms.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.1.1\">arXiv preprint arXiv:2309.13007</em>, 2023a.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et\\xa0al. (2023b)</span>\\n<span class=\"ltx_bibblock\">\\nChen, L., Zaharia, M., and Zou, J.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Frugalgpt: How to use large language models while reducing cost and improving performance.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.1.1\">arXiv preprint arXiv:2305.05176</em>, 2023b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chowdhery et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.\\xa0W., Sutton, C., Gehrmann, S., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Palm: Scaling language modeling with pathways.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.1.1\">arXiv preprint arXiv:2204.02311</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Diao et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nDiao, S., Wang, P., Lin, Y., and Zhang, T.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Active prompting with chain-of-thought for large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.1.1\">arXiv preprint arXiv:2302.12246</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Du et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nDu, Y., Li, S., Torralba, A., Tenenbaum, J.\\xa0B., and Mordatch, I.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Improving factuality and reasoning in language models through multiagent debate.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib8.1.1\">arXiv preprint arXiv:2305.14325</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dubois et\\xa0al. (2024)</span>\\n<span class=\"ltx_bibblock\">\\nDubois, Y., Galambosi, B., Liang, P., and Hashimoto, T.\\xa0B.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Length-controlled alpacaeval: A simple way to debias automatic evaluators.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.1.1\">arXiv preprint arXiv:2404.04475</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fu et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Complexity-based prompting for multi-step reasoning.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib10.1.1\">arXiv preprint arXiv:2210.00720</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guo et\\xa0al. (2024)</span>\\n<span class=\"ltx_bibblock\">\\nGuo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Deepseek-coder: When the large language model meets programming–the rise of code intelligence.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib11.1.1\">arXiv preprint arXiv:2401.14196</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hendrycks et\\xa0al. (2021)</span>\\n<span class=\"ltx_bibblock\">\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Measuring mathematical problem solving with the math dataset.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib12.1.1\">arXiv preprint arXiv:2103.03874</em>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang et\\xa0al. (2024)</span>\\n<span class=\"ltx_bibblock\">\\nHuang, Y., Feng, X., Li, B., Xiang, Y., Wang, H., Qin, B., and Liu, T.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Enabling ensemble learning for heterogeneous large language models with deep parallel collaboration.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib13.1.1\">arXiv preprint arXiv:2404.12715</em>, 2024.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et\\xa0al. (2024)</span>\\n<span class=\"ltx_bibblock\">\\nJiang, A.\\xa0Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.\\xa0S., de\\xa0Las\\xa0Casas, D., Hanna, E.\\xa0B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.\\xa0R., Saulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.\\xa0L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W.\\xa0E.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Mixtral of experts.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib14.1.1\">CoRR</em>, abs/2401.04088, 2024.\\n\\n</span>\\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.48550/ARXIV.2401.04088</span>.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.48550/arXiv.2401.04088\" title=\"\">https://doi.org/10.48550/arXiv.2401.04088</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nJiang, D., Ren, X., and Lin, B.\\xa0Y.\\n\\n</span>\\n<span class=\"ltx_bibblock\">LLM-blender: Ensembling large language models with pairwise ranking and generative fusion.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib15.1.1\">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.\\xa0 14165–14178, Toronto, Canada, July 2023. Association for Computational Linguistics.\\n\\n</span>\\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.18653/v1/2023.acl-long.792</span>.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://aclanthology.org/2023.acl-long.792\" title=\"\">https://aclanthology.org/2023.acl-long.792</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kojima et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nKojima, T., Gu, S.\\xa0S., Reid, M., Matsuo, Y., and Iwasawa, Y.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Large language models are zero-shot reasoners.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib16.1.1\">Advances in neural information processing systems</em>, 35:22199–22213, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liang et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nLiang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Encouraging divergent thinking in large language models through multi-agent debate.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib17.1.1\">arXiv preprint arXiv:2305.19118</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ling et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nLing, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Memisevic, R., and Su, H.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Deductive verification of chain-of-thought reasoning.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.1.1\">arXiv preprint arXiv:2306.03872</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lu et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nLu, K., Yuan, H., Lin, R., Lin, J., Yuan, Z., Zhou, C., and Zhou, J.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Routing to the expert: Efficient reward-guided ensemble of large language models, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI (2023)</span>\\n<span class=\"ltx_bibblock\">\\nOpenAI.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Gpt-4 technical report, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ouyang et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Training language models to follow instructions with human feedback.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib21.1.1\">Advances in neural information processing systems</em>, 35:27730–27744, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papineni et\\xa0al. (2002)</span>\\n<span class=\"ltx_bibblock\">\\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Bleu: a method for automatic evaluation of machine translation.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib22.1.1\">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA</em>, pp.\\xa0 311–318. ACL, 2002.\\n\\n</span>\\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.3115/1073083.1073135</span>.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://aclanthology.org/P02-1040/\" title=\"\">https://aclanthology.org/P02-1040/</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">RapidFuzz (2023)</span>\\n<span class=\"ltx_bibblock\">\\nRapidFuzz.\\n\\n</span>\\n<span class=\"ltx_bibblock\">python-levenshtein by rapidfuzz.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/rapidfuzz/python-Levenshtein\" title=\"\">https://github.com/rapidfuzz/python-Levenshtein</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Roziere et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nRoziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.\\xa0E., Adi, Y., Liu, J., Remez, T., Rapin, J., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Code llama: Open foundation models for code.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib24.1.1\">arXiv preprint arXiv:2308.12950</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shazeer et\\xa0al. (2017)</span>\\n<span class=\"ltx_bibblock\">\\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib25.1.1\">arXiv preprint arXiv:1701.06538</em>, 2017.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shnitzer et\\xa0al. (2024)</span>\\n<span class=\"ltx_bibblock\">\\nShnitzer, T., Ou, A., Silva, M., Soule, K., Sun, Y., Solomon, J., Thompson, N., and Yurochkin, M.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Large language model routing with benchmark datasets, 2024.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openreview.net/forum?id=LyNsMNNLjY\" title=\"\">https://openreview.net/forum?id=LyNsMNNLjY</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Team et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.\\xa0M., Hauth, A., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Gemini: a family of highly capable multimodal models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.1.1\">arXiv preprint arXiv:2312.11805</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">The Mosaic Research Team (2024)</span>\\n<span class=\"ltx_bibblock\">\\nThe Mosaic Research Team.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Introducing dbrx: A new state-of-the-art open llm.\\n\\n</span>\\n<span class=\"ltx_bibblock\">2024.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\" title=\"\">https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Touvron et\\xa0al. (2023a)</span>\\n<span class=\"ltx_bibblock\">\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Llama: Open and efficient foundation language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib29.1.1\">arXiv preprint arXiv:2302.13971</em>, 2023a.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Touvron et\\xa0al. (2023b)</span>\\n<span class=\"ltx_bibblock\">\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Llama 2: Open foundation and fine-tuned chat models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib30.1.1\">arXiv preprint arXiv:2307.09288</em>, 2023b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\\xa0al. (2024a)</span>\\n<span class=\"ltx_bibblock\">\\nWang, H., Polo, F.\\xa0M., Sun, Y., Kundu, S., Xing, E., and Yurochkin, M.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Fusing models with complementary expertise.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib31.1.1\">The Twelfth International Conference on Learning Representations</em>, 2024a.\\n\\n</span>\\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openreview.net/forum?id=PhMrGCMIRL\" title=\"\">https://openreview.net/forum?id=PhMrGCMIRL</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nWang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib32.1.1\">arXiv preprint arXiv:2305.04091</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\\xa0al. (2024b)</span>\\n<span class=\"ltx_bibblock\">\\nWang, Q., Wang, Z., Su, Y., Tong, H., and Song, Y.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib33.1.1\">arXiv preprint arXiv:2402.18272</em>, 2024b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Self-consistency improves chain of thought reasoning in language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib34.1.1\">arXiv preprint arXiv:2203.11171</em>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et\\xa0al. (2022)</span>\\n<span class=\"ltx_bibblock\">\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.\\xa0V., Zhou, D., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib35.1.1\">Advances in Neural Information Processing Systems</em>, 35:24824–24837, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et\\xa0al. (2023a)</span>\\n<span class=\"ltx_bibblock\">\\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Wizardlm: Empowering large language models to follow complex instructions.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib36.1.1\">arXiv preprint arXiv:2304.12244</em>, 2023a.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et\\xa0al. (2023b)</span>\\n<span class=\"ltx_bibblock\">\\nXu, X., Tao, C., Shen, T., Xu, C., Xu, H., Long, G., and Lou, J.-g.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Re-reading improves reasoning in language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib37.1.1\">arXiv preprint arXiv:2309.06275</em>, 2023b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yao et\\xa0al. (2023a)</span>\\n<span class=\"ltx_bibblock\">\\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.\\xa0L., Cao, Y., and Narasimhan, K.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Tree of thoughts: Deliberate problem solving with large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib38.1.1\">arXiv preprint arXiv:2305.10601</em>, 2023a.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yao et\\xa0al. (2023b)</span>\\n<span class=\"ltx_bibblock\">\\nYao, Y., Li, Z., and Zhao, H.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Beyond chain-of-thought, effective graph-of-thought reasoning in large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.1.1\">arXiv preprint arXiv:2305.16582</em>, 2023b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nYe, S., Kim, D., Kim, S., Hwang, H., Kim, S., Jo, Y., Thorne, J., Kim, J., and Seo, M.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Flask: Fine-grained language model evaluation based on alignment skill sets.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.1.1\">arXiv preprint arXiv:2307.10928</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nZhang, J., Xu, X., and Deng, S.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Exploring collaboration mechanisms for llm agents: A social psychology view.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib41.1.1\">arXiv preprint arXiv:2310.02124</em>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\\xa0al. (2022a)</span>\\n<span class=\"ltx_bibblock\">\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.\\xa0V., et\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Opt: Open pre-trained transformer language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib42.1.1\">arXiv e-prints</em>, pp.\\xa0 arXiv–2205, 2022a.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\\xa0al. (2022b)</span>\\n<span class=\"ltx_bibblock\">\\nZhang, Z., Zhang, A., Li, M., and Smola, A.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Automatic chain of thought prompting in large language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib43.1.1\">arXiv preprint arXiv:2210.03493</em>, 2022b.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zheng et\\xa0al. (2023)</span>\\n<span class=\"ltx_bibblock\">\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.\\xa0P., Zhang, H., Gonzalez, J.\\xa0E., and Stoica, I.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Judging llm-as-a-judge with mt-bench and chatbot arena.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.1.1\">arXiv preprint arXiv:2306.05685</em>, 2023.\\n\\n</span>\\n</li>\\n</ul>\\n</section>\\n<section class=\"ltx_appendix\" id=\"Ax1\">\\n<h2 class=\"ltx_title ltx_title_appendix\">Supplementary Material</h2>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A1\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Spearman Correlation using Different Similarity Functions</h2>\\n<div class=\"ltx_para\" id=\"A1.p1\">\\n<p class=\"ltx_p\" id=\"A1.p1.3\">We present results using TF-IDF-based similarity and Levenshtein similarity when calculating the Spearman correlation.\\nSpecifically, within each sample of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.1.m1.1\"><semantics id=\"A1.p1.1.m1.1a\"><mi id=\"A1.p1.1.m1.1.1\" xref=\"A1.p1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.p1.1.m1.1b\"><ci id=\"A1.p1.1.m1.1.1.cmml\" xref=\"A1.p1.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.p1.1.m1.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.p1.1.m1.1d\">italic_n</annotation></semantics></math> proposed answers,\\nwe calculate Spearman correlation coefficient between the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.2.m2.1\"><semantics id=\"A1.p1.2.m2.1a\"><mi id=\"A1.p1.2.m2.1.1\" xref=\"A1.p1.2.m2.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.p1.2.m2.1b\"><ci id=\"A1.p1.2.m2.1.1.cmml\" xref=\"A1.p1.2.m2.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.p1.2.m2.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.p1.2.m2.1d\">italic_n</annotation></semantics></math> similarity scores and the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.3.m3.1\"><semantics id=\"A1.p1.3.m3.1a\"><mi id=\"A1.p1.3.m3.1.1\" xref=\"A1.p1.3.m3.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.p1.3.m3.1b\"><ci id=\"A1.p1.3.m3.1.1.cmml\" xref=\"A1.p1.3.m3.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.p1.3.m3.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.p1.3.m3.1d\">italic_n</annotation></semantics></math> preference scores determined by the GPT-4-based evaluator.\\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A1.F6\" title=\"In Appendix A Spearman Correlation using Different Similarity Functions ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">6</span></a>, there is indeed a positive correlation between win rate and both TF-IDF similarity and Levenshtein similarity.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A1.F6\">\\n<div class=\"ltx_flex_figure\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"A1.F6.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"272\" id=\"A1.F6.1.g1\" src=\"x8.png\" width=\"364\"/>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"A1.F6.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"272\" id=\"A1.F6.2.g1\" src=\"x9.png\" width=\"367\"/>\\n</figure>\\n</div>\\n</div>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A1.F6.4.1.1\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" id=\"A1.F6.5.2\" style=\"font-size:90%;\">(a) Spearman Correlation using TF-IDF similarity; (b) Spearman Correlation using Levenshtein similarity.</span></figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A2\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>LLM Ranker</h2>\\n<div class=\"ltx_para\" id=\"A2.p1\">\\n<p class=\"ltx_p\" id=\"A2.p1.1\">This section introduces the setup of the LLM-Ranker used in this paper.\\nThe LLM-Ranker is designed to evaluate and rank the best output generated by some LLMs.\\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A2.T5\" title=\"In Appendix B LLM Ranker ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the template for prompting the model during these evaluations.\\nWe use this LLM-Ranker to pick the best answer among and use AlpacaEval evaluator\\nto evaluate the best ranked answer.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A2.T5\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Prompt for ranking with LLMs</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A2.T5.4\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.1.1\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"A2.T5.4.1.1.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.1.1.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.1.1.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.1.1.1.1.1.1\" style=\"font-size:90%;\">You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.2.2\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.2.2.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.2.2.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.2.2.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.2.2.1.1.1.1\" style=\"font-size:90%;\">I require a leaderboard for various large language models. I’ll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.3.3\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.3.3.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.3.3.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.3.3.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.3.3.1.1.1.1\" style=\"font-size:90%;\">## Instruction</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.4.4\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.4.4.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.4.4.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.4.4.1.1.1\" style=\"width:433.6pt;\"></span><pre class=\"ltx_verbatim ltx_font_typewriter\" id=\"A2.T5.4.4.4.1.1.2\" style=\"font-size:90%;\">\\n{\\n    \"instruction\": \"\"\"{instruction}\"\"\",\\n}</pre>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.5.5\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.5.5.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.5.5.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.5.5.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.5.5.1.1.1.1\" style=\"font-size:90%;\">## Model Outputs</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.6.6\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.6.6.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.6.6.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.6.6.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.6.6.1.1.1.1\" style=\"font-size:90%;\">Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.</span></span><pre class=\"ltx_verbatim ltx_font_typewriter\" id=\"A2.T5.4.6.6.1.1.2\" style=\"font-size:90%;\">\\n{\\n    {\\n        \"model_identifier\": \"{identifier_1}\",\\n        \"output\": \"\"\"{output_1}\"\"\"\\n    },\\n    {\\n        \"model_identifier\": \"{identifier_2}\",\\n        \"output\": \"\"\"{output_2}\"\"\"\\n    },\\n    {\\n        \"model_identifier\": \"{identifier_3}\",\\n        \"output\": \"\"\"{output_3}\"\"\"\\n    },\\n    {\\n        \"model_identifier\": \"{identifier_4}\",\\n        \"output\": \"\"\"{output_4}\"\"\"\\n    },\\n    {\\n        \"model_identifier\": \"{identifier_5}\",\\n        \"output\": \"\"\"{output_5}\"\"\"\\n    },\\n    {\\n        \"model_identifier\": \"{identifier_6}\",\\n        \"output\": \"\"\"{output_6}\"\"\"\\n    }\\n}</pre>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.7.7\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.7.7.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.7.7.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.7.7.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.7.7.1.1.1.1\" style=\"font-size:90%;\">## Task</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.8.8\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A2.T5.4.8.8.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.8.8.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.8.8.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.8.8.1.1.1.1\" style=\"font-size:90%;\">Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, …).</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A2.T5.4.9.9\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"A2.T5.4.9.9.1\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A2.T5.4.9.9.1.1\">\\n<span class=\"ltx_p\" id=\"A2.T5.4.9.9.1.1.1\" style=\"width:433.6pt;\"><span class=\"ltx_text\" id=\"A2.T5.4.9.9.1.1.1.1\" style=\"font-size:90%;\">## Best Model Identifier</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A3\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Case Study</h2>\\n<figure class=\"ltx_table\" id=\"A3.T6\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Case: Some models produce high quality answers.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T6.4\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T6.4.1.1.1\"><span class=\"ltx_text\" id=\"A3.T6.4.1.1.1.1\" style=\"font-size:90%;\">Role</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T6.4.1.1.2\"><span class=\"ltx_text\" id=\"A3.T6.4.1.1.2.1\" style=\"font-size:90%;\">Preference</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"A3.T6.4.1.1.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.1.1.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.1.1.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.1.1.3.1.1.1\" style=\"font-size:90%;\">Content</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T6.4.2.2.1\"><span class=\"ltx_text\" id=\"A3.T6.4.2.2.1.1\" style=\"font-size:90%;\">User</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T6.4.2.2.2\"><span class=\"ltx_text\" id=\"A3.T6.4.2.2.2.1\" style=\"font-size:90%;\">-</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T6.4.2.2.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.2.2.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.2.2.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.2.2.3.1.1.1\" style=\"font-size:90%;\">I want to talk about \"Smooth\", a song by Rob Thomas.</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.3.3\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T6.4.3.3.1\"><span class=\"ltx_text\" id=\"A3.T6.4.3.3.1.1\" style=\"font-size:90%;\">Qwen1.5-110B-Chat</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T6.4.3.3.2\"><span class=\"ltx_text\" id=\"A3.T6.4.3.3.2.1\" style=\"font-size:90%;\">0.35</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T6.4.3.3.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.3.3.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.3.3.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.3.3.3.1.1.1\" style=\"font-size:90%;\">… </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.3.3.3.1.1.2\" style=\"font-size:90%;border-color: #0000FF;\">It’s a perfect blend of rock, pop, and Latin music, showcasing Santana’s signature guitar work and Rob Thomas’s soulful vocals</span><span class=\"ltx_text\" id=\"A3.T6.4.3.3.3.1.1.3\" style=\"font-size:90%;\"> …</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.4.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.4.4.1\"><span class=\"ltx_text\" id=\"A3.T6.4.4.4.1.1\" style=\"font-size:90%;\">Qwen1.5-72B-Chat</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.4.4.2\"><span class=\"ltx_text\" id=\"A3.T6.4.4.4.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T6.4.4.4.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.4.4.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.4.4.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.4.4.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.5.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.5.5.1\"><span class=\"ltx_text\" id=\"A3.T6.4.5.5.1.1\" style=\"font-size:90%;\">Llama-3-70B-Instruct</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.5.5.2\"><span class=\"ltx_text\" id=\"A3.T6.4.5.5.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T6.4.5.5.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.5.5.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.5.5.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.5.5.3.1.1.1\" style=\"font-size:90%;\">… </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.5.5.3.1.1.2\" style=\"font-size:90%;border-color: #FF0000;\">it’s a collaboration between Rob Thomas, the lead vocalist of Matchbox Twenty, and Santana, the legendary guitarist</span><span class=\"ltx_text\" id=\"A3.T6.4.5.5.3.1.1.3\" style=\"font-size:90%;\"> …</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.6.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.6.6.1\"><span class=\"ltx_text\" id=\"A3.T6.4.6.6.1.1\" style=\"font-size:90%;\">WizardLM-2-8x22B</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.6.6.2\"><span class=\"ltx_text\" id=\"A3.T6.4.6.6.2.1\" style=\"font-size:90%;\">0.99</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T6.4.6.6.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.6.6.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.6.6.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.6.6.3.1.1.1\" style=\"font-size:90%;\">… </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.6.6.3.1.1.2\" style=\"font-size:90%;border-color: #FF8000;\">It spent 12 weeks at the top of the Billboard Hot 100 chart in the United States</span><span class=\"ltx_text\" id=\"A3.T6.4.6.6.3.1.1.3\" style=\"font-size:90%;\"> …\\n</span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.6.6.3.1.1.4\" style=\"font-size:90%;border-color: #FF8000;\">capturing the song’s sultry and energetic vibe</span><span class=\"ltx_text\" id=\"A3.T6.4.6.6.3.1.1.5\" style=\"font-size:90%;\"> …</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.7.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.7.7.1\"><span class=\"ltx_text\" id=\"A3.T6.4.7.7.1.1\" style=\"font-size:90%;\">Mixtral-8x22B-Instruct-v0.1</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.7.7.2\"><span class=\"ltx_text\" id=\"A3.T6.4.7.7.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T6.4.7.7.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.7.7.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.7.7.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.7.7.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.8.8\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.8.8.1\"><span class=\"ltx_text\" id=\"A3.T6.4.8.8.1.1\" style=\"font-size:90%;\">dbrx-instruc</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T6.4.8.8.2\"><span class=\"ltx_text\" id=\"A3.T6.4.8.8.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T6.4.8.8.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.8.8.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.8.8.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.8.8.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.9.9\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A3.T6.4.9.9.1\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T6.4.9.9.1.1\">\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.9.9.1.1.1\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"A3.T6.4.9.9.1.1.1.1\"><span class=\"ltx_text\" id=\"A3.T6.4.9.9.1.1.1.1.1\" style=\"font-size:90%;\">Aggregated</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T6.4.9.9.1.1.2\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"A3.T6.4.9.9.1.1.2.1\"><span class=\"ltx_text\" id=\"A3.T6.4.9.9.1.1.2.1.1\" style=\"font-size:90%;\">(Qwen1.5-110B-Chat)</span></td>\\n</tr>\\n</table>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A3.T6.4.9.9.2\"><span class=\"ltx_text\" id=\"A3.T6.4.9.9.2.1\" style=\"font-size:90%;\">0.99</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"A3.T6.4.9.9.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T6.4.9.9.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T6.4.9.9.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T6.4.9.9.3.1.1.1\" style=\"font-size:90%;\">\"Smooth\" is actually </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.9.9.3.1.1.2\" style=\"font-size:90%;border-color: #FF0000;\">a collaboration between Rob Thomas, who is the lead singer of the band Matchbox Twenty, and the renowned Latin rock guitarist Carlos Santana</span><span class=\"ltx_text\" id=\"A3.T6.4.9.9.3.1.1.3\" style=\"font-size:90%;\"> … </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.9.9.3.1.1.4\" style=\"font-size:90%;border-color: #0000FF;\">This iconic song blends rock, pop, and Latin music elements, showcasing Santana’s signature guitar work and Rob Thomas’s distinctive vocals</span><span class=\"ltx_text\" id=\"A3.T6.4.9.9.3.1.1.5\" style=\"font-size:90%;\"> … </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.9.9.3.1.1.6\" style=\"font-size:90%;border-color: #FF8000;\">The song was a colossal success, spending 12 weeks at the top of the Billboard Hot 100</span><span class=\"ltx_text\" id=\"A3.T6.4.9.9.3.1.1.7\" style=\"font-size:90%;\"> … The music video, </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T6.4.9.9.3.1.1.8\" style=\"font-size:90%;border-color: #FF8000;\">set in a vibrant club scene, complements the song’s sultry vibe</span><span class=\"ltx_text\" id=\"A3.T6.4.9.9.3.1.1.9\" style=\"font-size:90%;\"> …</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<figure class=\"ltx_table\" id=\"A3.T7\">\\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Case: all proposed responses are not good enough.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T7.4\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T7.4.1.1.1\"><span class=\"ltx_text\" id=\"A3.T7.4.1.1.1.1\" style=\"font-size:90%;\">Role</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T7.4.1.1.2\"><span class=\"ltx_text\" id=\"A3.T7.4.1.1.2.1\" style=\"font-size:90%;\">Preference</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"A3.T7.4.1.1.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.1.1.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.1.1.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.1.1.3.1.1.1\" style=\"font-size:90%;\">Content</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T7.4.2.2.1\"><span class=\"ltx_text\" id=\"A3.T7.4.2.2.1.1\" style=\"font-size:90%;\">User</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T7.4.2.2.2\"><span class=\"ltx_text\" id=\"A3.T7.4.2.2.2.1\" style=\"font-size:90%;\">-</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T7.4.2.2.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.2.2.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.2.2.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.2.2.3.1.1.1\" style=\"font-size:90%;\">How do you become an author?</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.3.3\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T7.4.3.3.1\"><span class=\"ltx_text\" id=\"A3.T7.4.3.3.1.1\" style=\"font-size:90%;\">Qwen1.5-110B-Chat</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T7.4.3.3.2\"><span class=\"ltx_text\" id=\"A3.T7.4.3.3.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T7.4.3.3.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.3.3.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.3.3.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.3.3.3.1.1.1\" style=\"font-size:90%;\">Becoming an author involves several steps and …\\n3. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.3.3.3.1.1.2\" style=\"font-size:90%;border-color: #0000FF;\">**Choose your niche**</span><span class=\"ltx_text\" id=\"A3.T7.4.3.3.3.1.1.3\" style=\"font-size:90%;\">:…\\n8. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.3.3.3.1.1.4\" style=\"font-size:90%;border-color: #0000FF;\">**Understand the publishing industry**</span><span class=\"ltx_text\" id=\"A3.T7.4.3.3.3.1.1.5\" style=\"font-size:90%;\">:…\\n10. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.3.3.3.1.1.6\" style=\"font-size:90%;border-color: #0000FF;\">**Self-publishing options**</span><span class=\"ltx_text\" id=\"A3.T7.4.3.3.3.1.1.7\" style=\"font-size:90%;\">:…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.4.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.4.4.1\"><span class=\"ltx_text\" id=\"A3.T7.4.4.4.1.1\" style=\"font-size:90%;\">Qwen1.5-72B-Chat</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.4.4.2\"><span class=\"ltx_text\" id=\"A3.T7.4.4.4.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T7.4.4.4.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.4.4.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.4.4.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.4.4.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.5.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.5.5.1\"><span class=\"ltx_text\" id=\"A3.T7.4.5.5.1.1\" style=\"font-size:90%;\">Llama-3-70B-Instruct</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.5.5.2\"><span class=\"ltx_text\" id=\"A3.T7.4.5.5.2.1\" style=\"font-size:90%;\">0.16</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T7.4.5.5.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.5.5.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.5.5.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.5.5.3.1.1.1\" style=\"font-size:90%;\">Becoming an author can be a rewarding and fulfilling career…\\n1. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.5.5.3.1.1.2\" style=\"font-size:90%;border-color: #FF0000;\">**Develop a love for reading and writing**</span><span class=\"ltx_text\" id=\"A3.T7.4.5.5.3.1.1.3\" style=\"font-size:90%;\">:…\\n10. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.5.5.3.1.1.4\" style=\"font-size:90%;border-color: #FF0000;\">**Market and promote your work**</span><span class=\"ltx_text\" id=\"A3.T7.4.5.5.3.1.1.5\" style=\"font-size:90%;\">:…\\n11. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.5.5.3.1.1.6\" style=\"font-size:90%;border-color: #FF0000;\">**Continuously improve**</span><span class=\"ltx_text\" id=\"A3.T7.4.5.5.3.1.1.7\" style=\"font-size:90%;\">:…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.6.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.6.6.1\"><span class=\"ltx_text\" id=\"A3.T7.4.6.6.1.1\" style=\"font-size:90%;\">WizardLM-2-8x22B</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.6.6.2\"><span class=\"ltx_text\" id=\"A3.T7.4.6.6.2.1\" style=\"font-size:90%;\">0.03</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T7.4.6.6.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.6.6.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.6.6.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.6.6.3.1.1.1\" style=\"font-size:90%;\">1. …\\n4. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.6.6.3.1.1.2\" style=\"font-size:90%;border-color: #FF8000;\">Finish your work</span><span class=\"ltx_text\" id=\"A3.T7.4.6.6.3.1.1.3\" style=\"font-size:90%;\">:…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.7.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.7.7.1\"><span class=\"ltx_text\" id=\"A3.T7.4.7.7.1.1\" style=\"font-size:90%;\">Mixtral-8x22B-Instruct-v0.1</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.7.7.2\"><span class=\"ltx_text\" id=\"A3.T7.4.7.7.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T7.4.7.7.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.7.7.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.7.7.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.7.7.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.8.8\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.8.8.1\"><span class=\"ltx_text\" id=\"A3.T7.4.8.8.1.1\" style=\"font-size:90%;\">dbrx-instruc</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T7.4.8.8.2\"><span class=\"ltx_text\" id=\"A3.T7.4.8.8.2.1\" style=\"font-size:90%;\">0.00</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"A3.T7.4.8.8.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.8.8.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.8.8.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.8.8.3.1.1.1\" style=\"font-size:90%;\">…</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.9.9\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A3.T7.4.9.9.1\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T7.4.9.9.1.1\">\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.9.9.1.1.1\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"A3.T7.4.9.9.1.1.1.1\"><span class=\"ltx_text\" id=\"A3.T7.4.9.9.1.1.1.1.1\" style=\"font-size:90%;\">Aggregated</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A3.T7.4.9.9.1.1.2\">\\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"A3.T7.4.9.9.1.1.2.1\"><span class=\"ltx_text\" id=\"A3.T7.4.9.9.1.1.2.1.1\" style=\"font-size:90%;\">(Qwen1.5-110B-Chat)</span></td>\\n</tr>\\n</table>\\n</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A3.T7.4.9.9.2\"><span class=\"ltx_text\" id=\"A3.T7.4.9.9.2.1\" style=\"font-size:90%;\">0.33</span></td>\\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"A3.T7.4.9.9.3\">\\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T7.4.9.9.3.1\">\\n<span class=\"ltx_p\" id=\"A3.T7.4.9.9.3.1.1\" style=\"width:260.2pt;\"><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.1\" style=\"font-size:90%;\">Becoming an author involves a dedicated journey…\\n1. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.2\" style=\"font-size:90%;border-color: #FF0000;\">**Cultivate a Love for Writing and Reading**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.3\" style=\"font-size:90%;\">:…\\n4. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.4\" style=\"font-size:90%;border-color: #0000FF;\">**Choose Your Niche**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.5\" style=\"font-size:90%;\">:…\\n5. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.6\" style=\"font-size:90%;border-color: #FF8000;\">**Finish Your Work**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.7\" style=\"font-size:90%;\">:…\\n10. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.8\" style=\"font-size:90%;border-color: #0000FF;\">**Self-Publishing**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.9\" style=\"font-size:90%;\">:…\\n11. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.10\" style=\"font-size:90%;border-color: #FF0000;\">**Marketing and Promotion**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.11\" style=\"font-size:90%;\">:…\\n12. </span><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"A3.T7.4.9.9.3.1.1.12\" style=\"font-size:90%;border-color: #FF0000;\">**Continuous Learning and Writing**</span><span class=\"ltx_text\" id=\"A3.T7.4.9.9.3.1.1.13\" style=\"font-size:90%;\">:…</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<div class=\"ltx_para\" id=\"A3.p1\">\\n<p class=\"ltx_p\" id=\"A3.p1.1\">We present a case study in this section.\\nDue to the length of the responses generated by all models, we will only show selected fragments for brevity.\\nTo illustrate how the aggregator synthesizes the response,\\nwe underlined similar expressions between the proposed responses and the aggregated response in different colors.\\nWe omit the content that all proposed responses have mentioned.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"A3.p2\">\\n<p class=\"ltx_p\" id=\"A3.p2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A3.T6\" title=\"In Appendix C Case Study ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">6</span></a> showcases the responses generated by different proposers.\\nThe aggregated response generated by Qwen1.5-110B-Chat\\nreflects a high preference for its own content but also incorporates key points\\nfrom Llama-3-70B-Instruct and WizardLM 8x22B.\\nNotably, GPT-4’s preference score for WizardLM 8x22B’s response is 0.99,\\nand the final aggregated answer also achieves a preference score of 0.99.</p>\\n</div>\\n<div class=\"ltx_para\" id=\"A3.p3\">\\n<p class=\"ltx_p\" id=\"A3.p3.1\">Meanwhile, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A3.T7\" title=\"In Appendix C Case Study ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">7</span></a> presents another case\\nwhere none of the proposed responses achieve a high GPT-4 preference score.\\nDespite this, the aggregator successfully identifies and\\nincorporates the strong points from these responses,\\nachieving a preference score of 0.33.</p>\\n</div>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A4\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>MATH Task</h2>\\n<div class=\"ltx_para\" id=\"A4.p1\">\\n<p class=\"ltx_p\" id=\"A4.p1.1\">Here, we demonstrate that our approach is applicable to reasoning tasks,\\nsuch as those in the MATH dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib12\" title=\"\">2021</a>)</cite>.\\nThe results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#A4.T8\" title=\"In Appendix D MATH Task ‣ Mixture-of-Agents Enhances Large Language Model Capabilities\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xa0<span class=\"ltx_text ltx_ref_tag\">8</span></a>, where we show that our method consistently enhances accuracy by a significant margin.\\nThis indicates that our approach is also effective for this type of task.\\nNotably, our method is complementary to existing reasoning techniques such as Chain of Thought <cite class=\"ltx_cite ltx_citemacro_cite\">Wei et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib35\" title=\"\">2022</a>)</cite> and Self-consistency <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\\xa0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.04692v1#bib.bib34\" title=\"\">2022</a>)</cite>.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A4.T8\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A4.T8.2.1.1\" style=\"font-size:90%;\">Table 8</span>: </span><span class=\"ltx_text\" id=\"A4.T8.3.2\" style=\"font-size:90%;\">\\nResults on the MATH task.\\nWe evaluate different aggregators,\\nwith all six models serving as proposers in each MoA layer.\\n</span></figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T8.4\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.4.1.1.1\">Aggregator</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.4.1.1.2\">Layer 1</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.4.1.1.3\">Layer 2</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.4.1.1.4\">Layer 3</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.4.2.1.1\">Qwen1.5-72B-Chat</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.4.2.1.2\">0.428</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.4.2.1.3\">0.526</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T8.4.2.1.4\">0.552</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.3.2.1\">Qwen1.5-110B-Chat</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.3.2.2\">0.500</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.3.2.3\">0.570</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.3.2.4\">0.576</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.4.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.4.3.1\">Wizard 8x22b</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.4.3.2\">0.544</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.4.3.3\">0.574</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.4.3.4\">0.580</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.5.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.5.4.1\">Mixtral-8x22B-Instruct-v0.1</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.5.4.2\">0.282</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.5.4.3\">0.534</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.5.4.4\">0.556</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.6.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.6.5.1\">Llama-3-70B-Instruct</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.6.5.2\">0.456</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.6.5.3\">0.584</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T8.4.6.5.4\">0.578</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T8.4.7.6\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T8.4.7.6.1\">dbrx-instruct</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T8.4.7.6.2\">0.314</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T8.4.7.6.3\">0.456</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T8.4.7.6.4\">0.522</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n</article>\\n</div>\\n<footer class=\"ltx_page_footer\">\\n<div class=\"ltx_page_logo\">Generated  on Fri Jun  7 06:57:42 2024 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_font_smallcaps\" style=\"position:relative; bottom:2.2pt;\">a</span>T<span class=\"ltx_font_smallcaps\" style=\"font-size:120%;position:relative; bottom:-0.2ex;\">e</span></span><span style=\"font-size:90%; position:relative; bottom:-0.2ex;\">XML</span><img alt=\"Mascot Sammy\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\\n</div></footer>\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ad416-52a7-42fc-a6ed-11781ac6f7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
